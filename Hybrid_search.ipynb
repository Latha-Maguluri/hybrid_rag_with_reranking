{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "91f5cf482d784e75b68b0af8af45ed86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d269b1eae6342f793e893bf58fa5664",
              "IPY_MODEL_cdf03142bf7f4ddb933bd3ca8e6a6519",
              "IPY_MODEL_d6f9e1c9e816431dbfd5abec80c26cd4"
            ],
            "layout": "IPY_MODEL_8324e1fcdded407abcc03718d3703b36"
          }
        },
        "8d269b1eae6342f793e893bf58fa5664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e750e09fd8ea419bad022c56a0b82863",
            "placeholder": "​",
            "style": "IPY_MODEL_20e1baa8f3aa46d8812e60ec36d2221e",
            "value": "Evaluating: 100%"
          }
        },
        "cdf03142bf7f4ddb933bd3ca8e6a6519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2c6dea68e2946a8b521538188d6be9f",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f0b73e9f40d4e8e87dc0dedc9e9ef7b",
            "value": 4
          }
        },
        "d6f9e1c9e816431dbfd5abec80c26cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8cacfaa7c6e4d02b9c973b2503ffd4d",
            "placeholder": "​",
            "style": "IPY_MODEL_421a88555243446f8e395c9dd83e1ab2",
            "value": " 4/4 [00:05&lt;00:00,  1.25s/it]"
          }
        },
        "8324e1fcdded407abcc03718d3703b36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e750e09fd8ea419bad022c56a0b82863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20e1baa8f3aa46d8812e60ec36d2221e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2c6dea68e2946a8b521538188d6be9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f0b73e9f40d4e8e87dc0dedc9e9ef7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8cacfaa7c6e4d02b9c973b2503ffd4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "421a88555243446f8e395c9dd83e1ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install --q chromadb rank_bm25 pypdf"
      ],
      "metadata": {
        "id": "grp4R5uSmdik"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dThDxgmHmUbS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2h-RvUtny8e",
        "outputId": "c54cf6b4-d58d-4ddb-8fb6-ea5c2a9a0c47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.53 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.55)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.75.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (0.3.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (2.11.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.53->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain_openai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain_openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain_openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.14-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain_openai\n",
            "Successfully installed langchain_openai-0.3.14 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "d8uz7Azlmc5m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJTFFcyAoI7I",
        "outputId": "de0c13fe-bd07-47b7-c5bb-1d43ac7bb9ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.55)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.33)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.22-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.22 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader =  PyPDFLoader(\"/content/rag.pdf\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "zH5ZVCTdn9sg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ANvlYKCaptGt",
        "outputId": "7f88ecad-b63f-4dc9-d682-87aabe183e0f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}, page_content='Retrieval-Augmented Generation for Natural Language\\nProcessing: A Survey\\nShangyu Wu\\nCity University of Hong Kong,\\nMBZUAI\\nYing Xiong*\\nMBZUAI\\nYufei Cui\\nMcGill University, Mila\\nHaolun Wu\\nMcGill University, Mila\\nCan Chen\\nMcGill University, Mila\\nYe Yuan\\nMcGill University, Mila\\nLianming Huang\\nCity University of Hong Kong\\nXue Liu\\nMBZUAI\\nTei-Wei Kuo\\nNational Taiwan University\\nNan Guan\\nCity University of Hong Kong\\nChun Jason Xue\\nMBZUAI\\nABSTRACT\\nLarge language models (LLMs) have demonstrated great success\\nin various fields, benefiting from their huge amount of parameters\\nthat store knowledge. However, LLMs still suffer from several key\\nissues, such as hallucination problems, knowledge update issues,\\nand lacking domain-specific expertise. The appearance of retrieval-\\naugmented generation (RAG), which leverages an external knowl-\\nedge database to augment LLMs, makes up those drawbacks of\\nLLMs. This paper reviews all significant techniques of RAG, espe-\\ncially in the retriever and the retrieval fusions. Besides, tutorial\\ncodes are provided for implementing the representative techniques\\nin RAG. This paper further discusses the RAG update, including\\nRAG with/without knowledge update. Then, we introduce RAG\\nevaluation and benchmarking, as well as the application of RAG in\\nrepresentative NLP tasks and industrial scenarios. Finally, this pa-\\nper discusses RAG’s future directions and challenges for promoting\\nthis field’s development.\\nReference Format:\\nShangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan,\\nLianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue.\\nRetrieval-Augmented Generation for Natural Language Processing: A Sur-\\nvey.\\n1 INTRODUCTION\\nLarge language models (LLMs) [77, 117, 123, 153, 182] have achieved\\nsignificant advancements in recent years and have become the\\ncornerstone of various applications in the field of natural language\\nprocessing (NLP). These LLMs are typically pre-trained on a large\\namount of natural language corpus and then fine-tuned on the\\nspecific downstream tasks’ datasets. Recent works [4, 57, 115, 127]\\ndemonstrate the success of LLMs can be explained by the fact that\\nLLMs act as knowledge bases, which refers to implicitly storing\\nthe learned knowledge in the parameters as internal memory and\\ngenerating responses by retrieving answers from memory. To store\\nmore knowledge for better generation performance, existing works\\n*Corresponding author.\\ngenerally enlarge the memory capacity by increasing the volume\\nof parameters [1, 12, 58, 85].\\nAlthough existing LLMs have shown great power, several chal-\\nlenges still hinder the development of LLMs. One of the most promi-\\nnent challenges is the hallucination problem [ 25, 75, 76], which\\nrefers to the tendency of LLMs to generate responses that are co-\\nherent and fluent but factually incorrect. Another big challenge\\nis the knowledge update issue. To update the knowledge stored\\nin the LLMs’ internal memory [ 115, 162, 186], it is necessary to\\nretrain/fine-tune LLMs with new data, which is a costly process.\\nAnother challenge for general LLMs is lacking domain-specific ex-\\npertise [22, 148, 149, 184]. Training a domain-specific LLM demands\\nconsiderable manpower for dataset collection.\\nTo address these challenges, recent works [11, 53, 95] have pro-\\nposed leveraging an external knowledge database to augment LLMs,\\nknown as Retrieval-Augmented Generation (RAG). By supplying\\nLLMs with retrieved relevant factual information, the hallucination\\nproblem can be alleviated to some extent. Besides, the knowledge\\nupdate issue can also be addressed by updating the external knowl-\\nedge database, which can augment LLMs with up-to-date knowl-\\nedge. RAG can also convert a general LLM into a domain-specific\\nLLM by constructing and utilizing a domain-specific knowledge\\ndatabase. Therefore, RAG plays an important role in augmenting the\\nfunctionality of LLMs, making them more accurate, knowledgeable,\\nand reliable in a wide range of applications.\\nContributions: This paper reviews all techniques involved in\\nRAG for natural language processing. Although there are several\\nsurvey papers for RAG [44, 63, 96, 180, 189], our survey still has\\nsome key insights,\\n(1) This paper systematically introduces each component of\\nRAG, including details about the retriever from building\\nto querying and techniques of the retrieval fusions with\\ntutorial codes.\\n(2) This paper exhibits different RAG training strategies, in-\\ncluding RAG with/without datastore update.\\n(3) This paper further discusses RAG evaluation and bench-\\nmarking, as well as the applications of RAG on downstream\\nNLP tasks and practical NLP scenarios.\\narXiv:2407.13193v3  [cs.CL]  1 Mar 2025'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}, page_content=\"Latent FusionLogits-based Fusion𝑞:what is the tallest mountain in the world?\\nRetriever\\n𝑎: Mount Everest\\nRetrievalsQuery-based FusionLayer-1Layer-2Layer-3Layer-n⋮EncoderIndexingDatastoreRetrieval Fusions𝑧ଵ: Mount Everest is the tallest mountain ...𝑧ଶ: The tallest mountain in the world ...𝑧ଷ: Mount Everest, located on the …𝑧ସ: Mount Everest's peak is the highest ...𝑧ହ: Mount Everest is best described ...𝑧ଵ, 𝑧ଶ, 𝑧ଷ, … , 𝑧\\u0bdeWhat is the tallest mountain …Inputs 𝑞Retrievals 𝑧ଵ, … , 𝑧\\u0bdeGenerators0.01, ⋯ , 0.95, ⋯ Logits 𝑙What is the tallest mountain …Mount Everest is the tallest mountain ...The tallest mountain in the world ...Mount Everest, located on the …\\n0.01, ⋯ , 0.95, ⋯ 0.00, ⋯ , 0.85, ⋯ 0.34, ⋯ , 0.45, ⋯ 0.33, ⋯ , 0.35, ⋯ 0.02, ⋯ , 0.93, ⋯ Logits 𝑙Logits 𝑙௭భ, 𝑙௭మ, 𝑙௭యGenerators⋮⋮Logits 𝑙\\u0be4Generators0.01, ⋯ , 0.98, ⋯ Logits 𝑙\\n⋮\\nInputs 𝑞Retrievals 𝑧ଵ, … , 𝑧\\u0bdeWhat is the tallest mountain …Inputs 𝑞𝑧ଵ, 𝑧ଶ, … , 𝑧\\u0bdeRetrievals 𝑧ଵ, … , 𝑧\\u0bdeℎ௭భ, ℎ௭మ, … , ℎ௭ೖEmbeddingsEncoder\\nFigure 1: The overview of retrieval-augmented generation for natural language processing. The inputs as queries are fed into\\nboth the retriever for retrieval knowledge and the generator for outputs. There are three kinds of retrieval fusions, including\\nquery-based fusion, logits-based fusion, and latent fusion.\\n(4) This paper finally identifies promising future directions for\\nexploring and the main challenges for addressing.\\nThe remainder of this paper is organized as follows. Section 2\\ngives an overview of RAG. Section 3 and Section 4 comprehensively\\nintroduce all technical details used in retrievers and retrieval fu-\\nsions. Section 6 presents how to train the RAG with/without new\\nknowledge. Section 8 presents the techniques used in representative\\nNLP tasks. Section 9 shows the applications of RAG in practical\\nNLP scenarios. Section 10 discusses the future directions of RAG.\\nSection 11 makes a final conclusion of this paper.\\n2 OVERVIEW OF RETRIEVAL-AUGMENTED\\nGENERATION\\nThis section gives an overview of Retrieval-Augmented Generation\\n(RAG) for NLP. An RAG system utilizes the external knowledge base\\n𝑫 to enhance the generation system. Taking external documents\\nas an example, 𝑫 consists of external documents, each of which\\ncontains a set of chunks 𝑐𝑖 ∈𝑪𝒊 . These chunks are transformed\\ninto vector embedding using an embedding model. When inputting\\na query 𝑞, which is embedded as a vector, then the retriever in\\nthe RAG system retrieves top-𝑘 chunks 𝑅𝑞 = {𝑟1,𝑟2,...,𝑟 𝑘}most\\nrelevant to the query 𝑞. The RAG system can use different fusion\\nmethods to fuse the retrieved chunks, which is discussed in Section\\n4. The overall process is formulated as follows:\\nRetriever(𝑞,𝐷)→ 𝑅𝑞 (1)\\nGenerator(Retrieval Fusion(𝑞,𝑅𝑞))→ 𝑎𝑛𝑠𝑤𝑒𝑟 (2)\\nAs shown in Figure 1, RAG typically consists of three modules, the\\nretriever, the generator, and retrieval fusions.\\nRetriever module usually comprises three components: an en-\\ncoder for encoding inputs into embeddings, an efficient indexing\\nthat supports approximate nearest neighbor search, and a datas-\\ntore for storing external knowledge in the form of key-value pairs.\\nThe main challenge in the retriever module is finding the optimal\\ntrade-off between retrieval efficiency and retrieval quality . The re-\\ntrieval efficiency refers to how fast the relevant information can\\nbe obtained, which involves accelerating encoding, efficient in-\\ndexing, batch querying in the datastore, etc. The retrieval quality\\nrefers to how relevant the information can be retrieved, which\\ninvolves chunk representation learning, advanced approximate\\nnearest neighbor search algorithms, etc.\\nRetrieval Fusions aims to leverage the retrieved information\\nto augment the generation. These fusion techniques can be catego-\\nrized into three major types: query-based fusion, latent fusion, and\\nlogits-based fusion. The query-based fusion augments inputs with\\nretrievals before feeding them into the generators. The logits-based\\nfusion focuses on the output logits of generators and fuses the\\nretrievals logits for more robust logits. The latent fusion refers to\\n2\"),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3'}, page_content='(a) Building the retriever.\\nIt met with positive\\nsales in Japan , and was\\npraised by both Japanese\\nand western critics . After\\nrelease , it received\\ndownloadable content ,\\nalong with an expanded\\nedition in November of\\nthat year . It was also\\nadapted into manga and\\nan original video\\nanimation series . …\\nThe game began\\ndevelopment in 2010 ,\\ncarrying over a large\\nportion of the work done\\non Valkyria Chronicles II .\\nWhile it retained the\\nstandard features of the\\nseries , it also underwent\\nmultiple adjustments ,\\nsuch as making the game\\nmore forgiving for series\\nnewcomers . …\\nThe game began development in 2010 …\\nWhile it retained the standard features …\\nIt met with positive sales in Japan , and …\\nAs with previous Valkyira Chronicles …\\nTroops are divided into five classes …\\nSource Documents Chunks\\n0.93 0.49 0.78 0.80 ⋯\\n0.22 0.49 0.97 0.92 ⋯\\n0.94 0.14 0.98 0.38 ⋯\\n0.70 0.87 0.23 0.56 ⋯\\n0.37 0.96 0.28 0.97 ⋯\\n0.83 0.75 0.33 0.65 ⋯\\n0.57 0.51 0.31 0.59 ⋯\\n0.98 0.93 0.64 0.54 ⋯\\n0.10 0.15 0.64 0.07 ⋯\\n0.39 0.68 0.01 0.10 ⋯\\n0.87 0.95 0.25 0.20 ⋯\\nEmbeddings\\nChunking Encoding\\nVector Database\\nIndexing\\nDatastore\\nBuilding\\n(b) Querying the retriever.\\nwhere did they film …\\nwho does annie …\\nwhen was puerto …\\nBatch Queries Encoding Query Embeddings\\n0.17 0.45 0.60 0.10 ⋯\\n0.24 0.55 0.37 0.62 ⋯\\n0.64 0.61 0.90 0.08 ⋯\\nANN Search\\nKey Ids\\n⋯\\n⋯\\n⋯\\nTop-K Neighbors\\n⋯\\n⋯\\n⋯\\nFinal Retrievals\\nPost-\\nprocessing504 519 995 𝑁𝑁1\\n1 𝑁𝑁2\\n1\\n𝑁𝑁1\\n2 𝑁𝑁2\\n2\\n𝑁𝑁1\\n3 𝑁𝑁2\\n3\\n𝑁𝑁1\\n1 𝑁𝑁5\\n1\\n𝑁𝑁2\\n2 𝑁𝑁1\\n2\\n𝑁𝑁3\\n3 𝑁𝑁2\\n3\\nFigure 2: Two stages of using the retriever.\\nintroducing retrieval representations into the latent representations\\nof generators, thus implicitly improving the models’ performance.\\nGenerator modules can be classified into two branches of gener-\\nators: default generators and retrieval-augmented (RA) generators.\\nThe default generators include most pre-trained/fine-tuned large\\nlanguage models, such as GPT-series models [ 12, 123, 129, 130],\\nMistral models [77], and Gemini-series models [5, 117, 135]. The\\nRA generators refer to the pre-trained/fine-tuned generators that\\nconsist of modules for fusing retrievals, such as RETRO [11, 157]\\nand Enc-Dec [101]. Those generators generate responses or make\\npredictions.\\nThe workflow of RAG involves three steps: 1) retrieving the\\nrelevant information from external databases based on given inputs;\\n2) fusing the retrieved information with inputs or intermediate\\nstates based on the fusion techniques; 3) making predictions by\\ngenerators based on the input and corresponding retrievals.\\n3 RETRIEVER\\nFigure 2 shows the two stages for using the retriever, which involves\\nfirst building the retriever and then querying the retriever. The\\nfollowing sections will introduce details about each stage.\\n3.1 Building the Retriever\\nThis section will explain how to build a retriever using a large natu-\\nral language corpus. As shown in Figure 2 (a), the process involves\\nthree steps: chunking corpus, encoding chunks, and building the\\nvector database. Specifically, building the vector database includes\\nbuilding the ANN index and storing the data with key-value pairs.\\n3.1.1 Chunking Corpus. Chunking techniques generally refer to\\ndividing large documents into small text chunks [11, 14, 46, 70, 120],\\nwhich is an indispensable key step in the process of building the\\nretriever. The intuitions behind chunking techniques are, (1) The\\ntexts or embeddings used for the indexing should be semantically\\nindependent, containing one core idea for models to encode. Short\\ntexts are more likely to be ambiguous, for example, the word “apple“\\ncan refer to a fruit or a company. (2) Encoding a long sequence\\ndocument would result in considerable resource overheads when\\nusing existing transformer-based models, while processing shorter\\ntext chunks can significantly accelerate the encoding process and\\nsave memory costs. Therefore, the main challenge of the chunking\\ntechniques is to find the best chunking size to make a better trade-\\noff between text semantics and encoding efficiency.\\nTo solve the above challenge, three key points need to be con-\\nsidered when determining the chunking size:\\n(1) Task’s preference. Different tasks may benefit from differ-\\nent kinds of retrieval chunks. For example, question-answer\\ntasks may prefer short phrases, while summarization tasks\\nmay prefer long documents.\\n(2) Encoder’s preference. Different encoder models have\\nvarying encoding capabilities on texts with different lengths.\\nFor example, models in the sentence-transformer [136] be-\\nhave better on a single sentence, while the text-embedding-\\nada-002 [122] is good at longer texts.\\n(3) Query’s preference. The length of the user’s queries should\\nbe aligned with the chunking size, which implicitly aligns\\nthe amount of contextual information in chunks with that in\\nqueries, thus improving the relevance between queries and\\nretrievals. For example, a retrieval database built on short\\nphrases may be useless for queries with long documents.\\nOverall, there is no golden rule for determining the chunking size,\\nand it depends on the specific RAG scenarios.\\nThere are basically three types of chunking techniques, includ-\\ning the chunking with fixed length , the semantic chunking ,\\nand the content-based chunking . Chunking with fixed length\\nis the simplest way to split documents sequentially using a length\\nhyperparameter. The semantic chunking cuts documents based on\\nsemantics, such as the period character or the newline character that\\nrepresents the end of the sentence. Existing state-of-the-art natural\\nlanguage processing toolkits, such as NLTK [121] and spaCy [35],\\n3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4'}, page_content='have provided convenient sentence-cutting methods. The content-\\nbased chunking segments documents according to the unique struc-\\ntural characteristics. For example, electronic medical records can\\nbe easily segmented based on the sections, or programming codes\\ncan be segmented based on function blocks.\\n3.1.2 Encoding Chunks. Encoding refers to numericalizing textual\\nchunks as vector representations (embeddings). These embeddings\\ngenerally capture the semantics of the chunks, enabling the retriever\\nto perform similarity searches based on content relevance rather\\nthan just keyword matching.\\nAccording to the sparsity of the embeddings, there are two kinds\\nof encoding methods, i.e., sparse encoding and dense encoding.\\nThe sparse encoding represents text by creating high-dimensional\\nvectors where most elements are zero. The basic sparse encoding\\nis one-hot encoding [ 54], which represents a word with a high-\\ndimensional vector as large as the vocabulary table size but only\\nmarks the value corresponding to the presence of the word as one.\\nThe embeddings produced by such encodings are called the one-hot\\nvector. Other common sparse encodings include:\\n(1) Bag of Words (BoW) [55]. This encoding improves one-\\nhot encoding by replacing the zero-one counting with the\\nfrequency counting. However, BoW ignores the syntax and\\nword order in the documents and focuses on statistical\\ninformation, thus only expressing limited semantics.\\n(2) Term Frequency-Inverse Document Frequency (TF-\\nIDF) [131]. This encoding not only counts the occurrence\\n(frequency) of each word but also adjusts these counts based\\non how common the word is across all documents (inverse\\ndocument frequency). TF-IDF helps emphasize words that\\nare more descriptive of the document’s content.\\n(3) BM25 [139] is a probabilistic ranking algorithm used in in-\\nformation retrieval to estimate the relevance of documents\\nto a search query by balancing term frequency, inverse\\ndocument frequency, and document length normalization,\\nensuring robust scoring even for long or short documents.\\nBM25 focuses on lexical matches and is computationally\\nefficient, making it a cornerstone of traditional search en-\\ngines.\\nSparse encoding is an efficient way to encode textual chunks. How-\\never, such encoding methods may not capture deeper semantic\\nmeanings well.\\nThe dense encoding generates vectors where each dimension can\\ncapture a range of semantic features, and most elements are non-\\nzero floating points. The dense embeddings are generally produced\\nby (deep) neural network models,\\n(1) BERT [31] and Variants. Bidirectional Encoder Represen-\\ntation from Transformers (BERT) is a typical pre-trained\\ntransformer model, generating dense semantic embeddings\\nthat capture the contextual information. Other BERT vari-\\nants, such as RoBERTa [107], DistilBERT [142], and ELEC-\\nTRA [21], further improve the semantic representations\\nwith advanced learning techniques.\\n(2) Siamese Encoders. This is a type of neural network de-\\nsigned to learn the similarity between inputs, which is\\nusually trained with contrastive learning. Existing state-\\nof-the-art siamese encoders are DPR [ 86], SimCSE [ 43],\\nContriever [71].\\n(3) LLM-based Encoders. This type of encoder benefits from\\nthe powerful representation capability of LLMs. LLMs, which\\ncontain billions of parameters and are pre-trained on vast\\namounts of data covering a wide range of topics, have ad-\\nvanced semantic language understanding capabilities. Typi-\\ncal LLM-based encoders are text-embedding-ada-002 [122],\\nbge-embedding [172], mxbai-embedding [144], MedCPT [83].\\nCompared to sparse encoding, dense encoding leverages deep neu-\\nral networks, especially transformers [ 155], to capture broader\\nlinguistic and semantic information. Dense encodings are widely\\nused in most representation scenarios. Moreover, some works also\\nutilized hybrid methods to encode text for leveraging both lexical\\nand semantic information [94, 109].\\n3.1.3 Building the Index. Indexing in the vector database aims to\\naccelerate the search process for data similar to high-dimensional\\nquery embedding. Unlike common indexing in databases, indexing\\nin the vector database mainly focuses on supporting efficient ap-\\nproximate nearest neighbor (ANN) search [34, 51, 84] rather than\\ntransaction operations like insertion, deletion, and update. The key\\nchallenge of indexing is making a good trade-off between search\\nquality and search efficiency. To solve the challenge, there are vari-\\nous specific optimizations in both algorithmic aspects and system-\\natic aspects to be explored, including choices of similarity metrics,\\ndimension reduction (DR) on embeddings, advanced ANN indexing,\\nsystem-level optimizations, hardware-aware optimization, and so\\non. Due to the page limits, this section discusses the optimizations\\nthat significantly affect the search quality and efficiency.\\nChoice of Similarity Metrics. The similarity metrics are the\\nbasic components in the retriever, which measures the degree of\\nrelevance between query embeddings and chunk embeddings. The\\nsimilarity metrics would affect the search quality. Typical simi-\\nlarity metrics include cosine similarity, Euclidean similarity, and\\nManhattan distance.\\nDimension Reduction on Embeddings. Reducing the dimen-\\nsionality of embeddings can improve search efficiency but at the\\nrisk of harming the semantic representations. The basic but effec-\\ntive dimension reduction (DR) is the principal component analysis\\n(PCA). The PCA is a simple statistical technique that transforms\\nthe original data into a new coordinate system while retaining the\\nmost important features. Another popular and advanced dimen-\\nsion reduction is locality-sensitive hashing (LSH). LSH significantly\\nreduces the dimensionality by mapping the data into buckets but\\npreserves the similarity of the original input data. The intuition\\nbehind LSH is that the nearest neighbors will be mapped into the\\nsame buckets. Unlike LSH, product quantization (PQ) [ 74] is an-\\nother popular and effective DR technique for ANN search. The core\\nidea of the PQ is to divide the high-dimensional space into smaller,\\nindependently quantized subspaces. Each subspace creates a code-\\nbook of different quantized integers to form the representative and\\ncompact vectors. The above techniques enable efficient storage and\\nfast approximate search but may lose semantic information. Recent\\nwork [19] proposed a new technique named AutoCompressor that\\n4'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='reduces the dimension of embeddings by compressing the original\\ncontext into semantically shorter embeddings.\\nAdvanced ANN Indexing. ANN Indexing generally refers to\\nthe methods or structures used to organize and manage data so that\\nthe approximate-nearest-neighbor search process is optimized for\\nretrieval quality and retrieval efficiency. This paper will introduce\\nseveral advanced ANN indexing techniques.\\n(1) The InVerted File system with Product Quantization\\n(IVFPQ) [34] is a simple but effective indexing framework\\nthat combines two powerful techniques to enable an effi-\\ncient and scalable ANN search process. The main idea of\\nIVFPQ is first to cluster the data for coarse-grained partition\\nand then to compress the data within each cluster into sub-\\nvectors for fine-grained quantization. The coarse-grained\\nclustering (the IVF component) significantly reduces the\\nsearch space, while the fine-grained quantization (the PQ\\ncomponent) ensures a high retrieval performance.\\n(2) The Hierarchical Navigable Small World (HNSW) [111]\\nuses a hierarchical graph structure to perform ANN search\\nin high-dimensional spaces efficiently. Specifically, HNSW\\ntreats high-dimensional vectors as nodes and connects them\\nwith their nearest neighbors. The multi-layer graph struc-\\nture is determined probabilistically to ensure fewer nodes\\nat higher layers for efficient search.\\n(3) Tree-based Indexing aims to organize high-dimensional\\nvectors in tree-liked structures, such as KD-Trees [134], Ball\\nTrees [66] and VP-Trees [106]. Typical tree-based indexing\\nis Approximate Nearest Neighbors Oh Yeah (Annoy) [150],\\nwhich uses a forest of trees built based on random projec-\\ntions to separate the vector space into multiple hyperplanes\\nfor efficient ANN search.\\n3.1.4 Building the Datastore with Key-Value Pairs. The datastore\\nused in the vector database is a specialized database that stores\\nand manages data as a collection of key-value pairs, where keys\\nare the unique identifiers of high-dimensional embeddings and\\nvalues are the domain-specific knowledge. Since the amount of\\nthe data stored in the datastore may be quite large, the storage\\nengine, such as LMDB [108] or RocksDB [37], should be capable\\nof efficient retrieval and data persistence. The key point in the\\ndatastore for ANN search is what should be used to be stored\\nas values. For example, for question-answer tasks, when adding\\nretrievals to prompts, the naive but effective way is to store the\\nquestion embedding as the key and question-answer pairs as the\\nvalue. This can help the generation process as retrievals are used as\\ndemonstrations for models. Recent works have proposed various\\nstate-of-the-art vector databases including the indexing and the\\ndatastore, such as Milvus [50, 159], FAISS [34, 84], LlamaIndex [103],\\netc.\\n3.1.5 Code Demonstrations. Algorithm 3.1 shows detailed steps to\\nbuild the retriever. Lines 2-8 present the chunking and the encoding\\nprocess for a natural language corpus containing multiple docu-\\nments. In line 6, algorithm 3.1 takes the concatenation of the current\\nchunk and the next chunk as the value. Notably, the choice of value\\ncan vary for different tasks. Another practical issue is that the mem-\\nory cost of all keys and values may exceed the memory capacity of\\nAlgorithm 3.1 Building the retriever.\\nInput: A natural language corpus𝐷 = {𝑑1,...,𝑑 𝑛}for building the knowl-\\nedge database, an encoder Efor encoding chunks.\\nOutput: The index Iand the key-value store S.\\n1: K= {},V= {};\\n2: for 𝑑𝑖 ∈𝐷do\\n3: 𝑐1\\n𝑖,...,𝑐 𝑚\\n𝑖 = 𝐶ℎ𝑢𝑛𝑘(𝑑𝑖); /* Split each data 𝑑𝑖 */\\n4: for 𝑗 from 1 to 𝑚do\\n5: 𝑒𝑗\\n𝑖 = E(𝑐𝑗\\n𝑖); /* Encode each chunk 𝑐𝑗\\n𝑖 */\\n6: Add 𝑒𝑗\\n𝑖 into Kand 𝑐𝑗\\n𝑖 +𝑐𝑗+1\\n𝑖 into V; /* Take next chunk as an\\nexampless */\\n7: The Kand Vpersist in the storage (e.g., SSD) if necessary;\\n8: end for\\n9: end for\\n10: Build the index Iwith K;\\n11: Store Kand Vinto the key-value store S;\\n12: return Iand S;\\nAlgorithm 3.2 Query the retriever.\\nInput: A query input 𝑞, an encoder Efor encoding chunks, the index I,\\nthe key-value store S, the parameter 𝑘.\\nOutput: Top-𝑘nearest neighbor knowledge.\\n1: 𝑒 = E(𝑞);\\n2: {𝑖𝑑𝑥1,...,𝑖𝑑𝑥 𝑘}= I.𝑆𝑒𝑎𝑟𝑐ℎ(𝑒,𝑘); /* Search the top- 𝑘nearest\\nneighbors */\\n3: {𝑣1,...,𝑣 𝑘}= S.𝐹𝑒𝑡𝑐ℎ ({𝑖𝑑𝑥1,...,𝑖𝑑𝑥 𝑘}); /* Fetch the values of the\\nneighbors */\\n4: {𝑣𝑗1 ,...,𝑣 𝑗𝑘}= 𝑃𝑜𝑠𝑡𝑃𝑟𝑜𝑐𝑒𝑠𝑠 ({𝑣1,...,𝑣 𝑘})\\n5: return {𝑣𝑗1 ,...,𝑣 𝑗𝑘};\\nthe server in the practical scenario. Thus, it is recommended that\\nthe keys and values persist in the storage if necessary.\\n3.2 Querying the Retriever\\nThis section will explain how to query the pre-built retriever, which\\nbasically includes three steps as shown in Figure 2(b): encoding\\nqueries, ANN search, and post-processing.\\n3.2.1 Encoding Queries and ANN Search. To align with the pre-\\nbuilt embedding space, the retriever uses the same encoder to en-\\ncode queries during the querying stage. The ANN search leverages\\nthe pre-built indexing and datastore to find similar data via ANN\\nsearching algorithms and then retrieves the corresponding values.\\nSearching the index refers to searching the pre-built index,\\nfinding the top-k nearest neighbors, and returning the unique iden-\\ntifiers of k nearest neighbors. The nearest neighbor search process\\ndepends on indexing algorithms or structures. Taking IVFPQ as an\\nexample, the search process first compares the query embedding\\nwith cluster embeddings and selects several candidate clusters for\\nfurther search. Then, within each cluster, the search process per-\\nforms the same product quantization on the query embedding and\\nfinds the top-k nearest neighbors based on the distance. Finally, the\\nsearch process merges all nearest neighbor candidates and re-orders\\nall candidates for the final top-k nearest neighbors.\\nRetrieving values from datastore fetches the corresponding\\nvalues based on the nearest key identifiers.\\n5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6'}, page_content='Retrieval fusions in RAG\\nQuery-based\\nFusions\\nLogits-based\\nFusions Latent Fusions\\nText Concatenation Feature Concatenation\\nREALM [53]\\nRAG [95]\\nREINA [161]\\nRALM [133]\\nFID [72]\\nRETRO-\\nPROMPT [16]\\nLUMEN [27]\\nEnsemble Calibration\\nkNN-LM [88]\\nkNN-MT [87]\\nkNN-Adapter [68]\\nRobust-kNN-MT [78]\\nSource-Context [97]\\nAttention Weighted Addition\\nRETRO [11]\\nEnc-Dec [101]\\nLONGMEM [163]\\nEAE [40]\\nReFusion [169]\\nFigure 3: The categories of fusion methods in RAG.\\nAlgorithm 4.1 Query-based Fusions.\\nInput: A query input𝑞, top-𝑘nearest neighbor knowledge{𝑣1,...,𝑣 𝑘}, an\\nencoder E𝑓 and a decoder D𝑓 for feature concatenation, the generator\\nGfor text concatenation.\\nOutput: Generated response 𝑦.\\n1: if Use the text concatenation then\\n2: 𝑥 = 𝑣1 ⊕... ⊕𝑣𝑘 ⊕𝑞; /* Concatenate neighbor texts and query */\\n3: 𝑦= G(𝑥);\\n4: else\\n5: 𝑒𝑞 = E𝑓 (𝑞),𝑒𝑣𝑗 = E𝑓 (𝑣𝑗),𝑗 ∈{1,...,𝑘 };\\n6: 𝑒𝑥 = 𝑒𝑞 ⊕𝑒𝑣1 ⊕... ⊕𝑒𝑣𝑘; /* Concatenate embeddings of neighbors\\nand query */\\n7: 𝑦= D𝑓 (𝑒𝑥)\\n8: end if\\n9: return 𝑦;\\n3.2.2 Post-Processing. The post-processing involves a set of tech-\\nniques after the initial retrieval step. These techniques aim to refine,\\nenhance, or adapt the retrievals based on the specific task objectives.\\nThis section will list some typical post-processing techniques.\\nReranking aims to reorder the retrieved knowledge based on\\ntask-specific objectives. The intuition is that the knowledge is re-\\ntrieved based on task-agnostic metrics, such as Euclidean distance.\\nExisting reranking methods [20, 60, 92, 156] mostly design different\\narchitectures or strategies to reorder the retrieved knowledge.\\n3.2.3 Code Demonstrations. After building the retriever, this sec-\\ntion demonstrates the detailed steps of querying the retriever to\\nobtain the top-𝑘 nearest neighbor knowledge in algorithm 3.2, in-\\ncluding encoding the query (line 1), performing the approximate\\nnearest neighbor search (line 2), and fetching the knowledge for\\nfusion (line 3). These three steps depend on the specific APIs of en-\\ncoders, indexing, and datastore. After obtaining the top-𝑘retrievals,\\noptimizations for post-processing are applied (line 4).\\n4 RETRIEVAL FUSIONS\\nRetrieval fusions refer to how to leverage the retrieved knowledge\\nto improve generators’ performance. Basically, there are three types\\nof retrieval fusions: query-based fusions, logits-based fusions, and\\nlatent fusions. Figure 3 shows the detailed categorization of fusions\\nand representative works of each retrieval fusion in RAG.\\n4.1 Query-based Fusion\\nThe simplest and most direct fusion technique is query-based fu-\\nsion, which integrates the retrieved information with input queries\\nto generate responses. The query-based fusion can be further cate-\\ngorized into two sub-classes according to the type of concatenated\\ninformation, i.e., text concatenation and feature concatenation.\\nText concatenation involves performing query-based fusion with\\nraw texts, making it particularly suitable for contemporary LLMs\\nlike GPT-4. These models function as black-box systems with lim-\\nited interaction capabilities, typically offering only API access to\\nusers. Existing works [53, 95, 133] directly concatenate the input\\nwith the top-𝑘 retrieved sentences/documents to form the query\\nfor generators. To better use the in-context learning capability of\\nLLMs, some works [36, 98, 156, 161] design effective prompt tem-\\nplates to integrate retrieved information and inputs. To address the\\nissue of lengthy inputs after concatenating retrievals, recent stud-\\nies [6, 104, 110, 166, 176] have introduced methods for assigning\\nimportance weights to elements within the retrieved knowledge\\nbase and filtering out less relevant contexts based on these weights.\\nThe feature concatenation involves merging the encoded re-\\ntrievals with the input features. A simple yet effective approach\\nis FID [72], which first encodes the retrieved passages into sparse\\nor dense representations and then takes the concatenated features\\nas the input for a generator. The state-of-the-art performance of\\nthe FID demonstrates the efficacy of feature concatenation. The\\nfollow-up works [27, 52, 73, 105, 141] further improve the FID by\\njointly tuning the retriever and the encoder, which can enhance\\nthe retrieved knowledge’s representations. Besides, Chen et al. [16]\\nconcatenate the representations of related knowledge as demon-\\nstrations for prompt learning, yielding better generalization.\\nAlgorithm 4.1 presents how to leverage query-based fusions to\\nfuse retrieved knowledge. For those using text concatenation [53,\\n133], algorithm 4.1 first concatenates the retrieved texts and inputs\\n(line 2), then feeds the concatenated input into the generator. No-\\ntably, since there is a limit to the maximum input length of existing\\nlanguage models, concatenating too many retrievals would result\\nin a truncation of the concatenated input, which may cut the given\\ninput. Therefore, designing the prompt template is the key step for\\nthis branch of work. For those using feature concatenation [52, 72],\\nalgorithm 4.1 first leverages an encoder to obtain the feature (line\\n5), then concatenates the feature of input and retrievals (line 6),\\nfinally passes the concatenated feature into a decoder model (line\\n6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}, page_content='Algorithm 4.2 Logits-based Fusions.\\nInput: A query input 𝑞, top-𝑘nearest neighbor knowledge {𝑣1,...,𝑣 𝑘},\\nthe generator G.\\nOutput: Generated response 𝑦.\\n1: 𝑦𝑞 = G(𝑞);\\n2: for 𝑗 from 1 to 𝑘do\\n3: 𝑦𝑣𝑗 = G(𝑣𝑗)\\n4: end for\\n5: if Use ensemble then\\n6: 𝑦= 𝜆Í\\n𝑗 𝑦𝑣𝑗 +(1 −𝜆)𝑦𝑞;\\n7: else\\n8: 𝜆𝑡 = 𝐶𝑎𝑙𝑖𝑏𝑟𝑎𝑡𝑒 (𝑦𝑞,𝑦𝑣1 ,...,𝑦 𝑣𝑘)\\n9: 𝑦= 𝜆𝑡\\nÍ\\n𝑗 𝑦𝑣𝑗 +(1 −𝜆𝑡)𝑦𝑞;\\n10: end if\\n11: return 𝑦;\\n7). This branch of work generally incurs high memory costs due to\\nthe long sequence length.\\n4.2 Logits-based Fusion\\nThe logits-based fusion refers to incorporating the retrieved knowl-\\nedge into the output layers. Basically, retrieved knowledge would\\nbe fed into the same model to obtain the logits for enhancing\\nor calibrating the predictions. Therefore, logits-based fusion can\\nbe categorized into two branches, i.e., ensemble-based fusion and\\ncalibration-based fusion.\\nEnsemble-based fusion treats the logits from the retrieved knowl-\\nedge as part of an ensemble of predictions. Such ensemble-based\\nfusion can significantly improve the generalization and robustness\\nof the model [ 87, 88, 175]. One notable work of ensemble-based\\nfusion is kNN-LM [88], which aggregates the logits of the top- 𝑘\\nnearest neighbors’ targets and then interpolates the final predic-\\ntions. Similar to kNN-LM, Khandelwal et al. [87] propose kNN-MT\\nto enhance the machine translation using retrievals’ logits, which\\nis also followed by a branch of works [68, 190].\\nDifferent from ensemble-based fusion, calibration-based fusion\\nuses the logits from the retrieved knowledge as a form of calibration\\nfor the model’s predictions. Specifically, Jiang et al. [78] propose a\\nconfidence-enhanced kNN-MT that refines the kNN distribution\\nand interpolation weights with the neural machine translation\\nconfidence. Li et al. [97] propose to leverage the source context to\\ncalibrate the retrieval-augmented neural machine translation.\\nAlgorithm 4.2 demonstrates the detailed steps of using the logits-\\nbased fusion to integrate the retrieved knowledge. This branch of\\nwork first treats retrievals as similar data to augment the model\\n(lines 2-4). For ensemble, algorithm 4.2 leverages a hyperparameter\\nto fuse the retrieval logits and the output logits (line 6). For calibra-\\ntion, algorithm 4.2 dynamically determines the parameter based on\\nthe retrieval logits and the output logits (line 8). Then, algorithm 4.2\\nperforms the same fusion with the computed parameter (line 9).\\n4.3 Latent Fusion\\nThe latent fusion investigates merging the retrieved knowledge\\ninto the hidden states of generators for a better generation. Based\\non the introduction method, latent fusion can be further classified\\ninto two categories: attention-based and weighted-addition.\\nAlgorithm 4.3 Latent Fusions.\\nInput: A query input 𝑞, top-𝑘nearest neighbors {𝑣1,...,𝑣 𝑘}, the encoder\\nE, the generator Gcontaining 𝑙 pairs of modules {(M𝐴\\n1 ,M𝐹\\n1 ),... },\\nwhere M𝐴\\n𝑖 and M𝐹\\n𝑖 are the attention module and the FFN module\\nat layer 𝑖, M𝐶\\n𝑖 is the cross-attention module used in attention-based\\nlatent fusions.\\nOutput: Generated response 𝑦.\\n1: if Use the attention then\\n2: ℎ𝐹\\n0 = 𝑞;\\n3: for 𝑖from 1 to 𝑙 do\\n4: ℎ𝐴\\n𝑖 = M𝐴\\n𝑖 (ℎ𝐹\\n𝑖−1 );\\n5: 𝑒𝑣1 ,...,𝑒 𝑣𝑘 = E(𝑣1,...,𝑣 𝑘,ℎ𝐴\\n𝑖 )\\n6: ℎ𝑅\\n𝑖 = M𝐶\\n𝑖 (ℎ𝐴\\n𝑖 ,𝑒𝑣1 ,...,𝑒 𝑣𝑘); /* Cross-attention */\\n7: ℎ𝐹\\n𝑖 = M𝐹\\n𝑖 (ℎ𝑅\\n𝑖 )\\n8: end for\\n9: 𝑦= 𝐿𝑀_𝐻𝐸𝐴𝐷(ℎ𝐹\\n𝑙 )\\n10: else\\n11: 𝑒𝑣1 ,...,𝑒 𝑣𝑘 = E(𝑣1,...,𝑣 𝑘)\\n12: ℎ𝐹\\n0 = 𝑞;\\n13: for 𝑖from 1 to 𝑙 do\\n14: ℎ𝐴\\n𝑖 = M𝐴\\n𝑖 (ℎ𝐹\\n𝑖−1 );\\n15: ℎ𝑅\\n𝑖 = ℎ𝐴\\n𝑖 +1\\n𝑘\\nÍ\\n𝑗 𝑤𝑗𝑒𝑣𝑗 /* Weighted sum */\\n16: ℎ𝐹\\n𝑖 = M𝐹\\n𝑖 (ℎ𝑅\\n𝑖 )\\n17: end for\\n18: 𝑦= 𝐿𝑀_𝐻𝐸𝐴𝐷(ℎ𝐹\\n𝑙 )\\n19: end if\\n20: return 𝑦;\\nOne notable contribution of attention-based fusion is the Retrieval-\\nEnhanced Transformer (RETRO) [11]. RETRO represents a pioneer-\\ning effort in pre-training retrieval-based LLMs, introducing a new\\ncross-attention module to integrate retrieved knowledge directly\\ninto the model’s hidden states. A significant finding from this work\\nis demonstrating a scaling law for the retrieval database, where\\nRETRO, with a 2 trillion token database, attains performance compa-\\nrable to that of major models like GPT-3 and Jurassic-1, albeit with\\n25 times fewer parameters. Customizing the transformer model in\\nRETRO highlights the potential of pre-trained, retrieval-enhanced\\narchitectures in improving the efficiency and scalability of LLMs.\\nIn addition to RETRO, other studies [13, 28, 101, 167, 170] have\\ncontributed to the field by leveraging new attention modules to in-\\ntroduce external knowledge. Typically, Li et al. [101] have extended\\nthe RETRO model by decoupling the context encoding from the\\nmodel inference. Wu et al. [170], Wang et al. [163] store the hidden\\nattention keys and values into external memory and retrieve the\\nknowledge from the memory using an attention mechanism.\\nDue to the high complexity of the attention mechanism, another\\nbranch of work adopts lightweight (weighted) additions to intro-\\nduce retrieved knowledge. Fevry et al. [40] propose the EAE model\\nthat retrieves top-𝑘 related entities’ embeddings from a learnable\\nexternal memory and adds entities’ embeddings to the hidden states\\nof the model. Wu et al. [ 169] propose ReFusion, which explores\\nvarious learnable reranking schemes to first re-weight the retrieved\\nknowledge’s embeddings, then use weighted addition to incorpo-\\nrate them into the hidden states of the model. Those approaches\\nsignify a growing trend towards models that dynamically select\\n7'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8'}, page_content='and integrate relevant knowledge, paving the way for more sophis-\\nticated and nuanced language generation and understanding.\\nAlgorithm 4.3 shows the steps of using latent fusion to introduce\\nthe retrieved knowledge into the hidden states of the generator.\\nFor attention-based latent fusion, algorithm 4.3 first encodes the re-\\ntrievals with the output states of the attention module (line 5), then\\nuses a cross-attention module to fuse the retrieval features into the\\nhidden state (line 6). Different from attention-based latent fusion,\\nweighted-addition-based latent fusion adopts a more lightweight\\nway to incorporate retrieved knowledge (lines 10-19). Algorithm 4.3\\nfirst encodes the retrievals before feeding them into the generator\\n(line 11), which can be done offline and directly stored as values in\\nthe datastore. Then, algorithm 4.3 learns a set of weights to add the\\nretrieval features on the hidden states of generators (line 15).\\n4.4 Comparison of Different Fusion Methods\\nEach fusion method exhibits distinct advantages and limitations.\\nThe query-based fusion method is a simple and straightforward\\napproach that preserves the raw information from retrieved in-\\nformation. However, it significantly increases the input sequence\\nlength, resulting in higher computational costs. Additionally, this\\nkind of method suffers from limited contextual integration capabil-\\nities and scalability, as it struggles to effectively combine retrieved\\ninformation with the input context.\\nThe logits-based fusion method combines the output logits (or\\nprobabilities) of the base LLM with those from a separate model\\n(or shared LLM) that processes the retrieved information. This ap-\\nproach is computationally efficient, as it can process retrievals and\\ninputs in batches, avoiding increasing the input sequence length.\\nIt decouples the retrieval and generation processes, enabling inde-\\npendent improvements to each component. Nevertheless, this kind\\nof method is limited by its shallow contextual integration, as fusion\\noccurs only at the output level. Consequently, it may underperform\\nin complex reasoning tasks that require fine-grained interaction\\nbetween the input and retrieved information.\\nIn contrast, the latent fusion method integrates retrieved informa-\\ntion directly into the hidden states (or intermediate representations)\\nof the LLM. This approach is both scalable and efficient, as it avoids\\nexpanding the input sequence length. Moreover, it facilitates deep,\\ncontext-aware integration of retrieved information and supports\\nfine-grained control over how the information is fused. However,\\nthe latent fusion method necessitates significant architectural mod-\\nifications and careful training to achieve optimal performance. In\\nsummary, the choice of fusion method depends on the specific ap-\\nplication, available resources, and the desired trade-off between\\nsimplicity and performance. Each method offers unique strengths\\nand weaknesses, making them suitable for different use cases and\\noperational constraints.\\n5 GENERATORS\\nThis section introduces representative generators, which are gener-\\nally pre-trained on large datasets. Existing generators are mostly\\nlarge language models that adopt or modify the transformer-based\\narchitecture [155]. For example, DeepSeek [49], Llama-series mod-\\nels [153, 154], GPT-series models [12, 123, 129, 130], and Gemini-\\nseries models [5, 117, 135] remove all encoder modules, retaining\\nonly the decoder module, which includes an attention module and a\\nfeed-forward network module. Other advanced techniques, such as\\nroot mean square layer normalization [183], rotary position embed-\\nding [151], and group query attention mechanisms [3], have been\\nincorporated into the design of existing large language models to\\nenhance their performance.\\nExisting generators can be generally categorized into two groups,\\nclose-sourced LLMs [12, 123] and open-sourced LLMs [49, 153, 154].\\nThe formers can only adopt query-based fusions to introduce exter-\\nnal knowledge. While the latter can adapt to all kinds of fusions. Es-\\npecially generators using the latent fusion typically would train the\\ngenerators from scratch. Those generators integrate novel modules\\ndesigned for retrieved information fusion, such as cross-attention-\\nbased module [ 11, 101, 170]. Their training paradigm combines\\npre-training on massive textual datasets with integrating external\\nretrieved knowledge to establish robust information grounding. As\\ndetailed in Section 4.3, those generators enable dynamic, context-\\naware retrieved information incorporation while further improving\\nthe LLM’s generative capabilities.\\n6 RAG TRAINING AND DATASTORE UPDATE\\nThis section introduces RAG training, which can be categorized\\ninto two main classes: RAG without datastore update and RAG\\nwith datastore update . The former refers to the case where only\\ntrainable parameters in each module of RAG would be updated, and\\nthe knowledge in the datastore would remain the same during the\\ntraining stage. The latter refers to the case where the knowledge in\\nthe datastore would be updated, then each module’s parameters in\\nRAG would be updated in a similar way as the former case.\\n6.1 RAG without Datastore Update\\nThe goal of training RAG without datastore update is to update the\\nknowledge stored in the short-term memory of generators based\\non the existing knowledge datastore. As shown in Figure 4 (a)-(c),\\nthere are three training cases, i.e., training the retriever, training\\nthe generator, and jointly training the retriever and generator.\\n6.1.1 Training retriever. Considering the case of no datastore up-\\ndate, training the retriever generally refers to training the retriever\\nencoder and rebuilding the indexing. Since sparse encodings rely on\\nstatistical methods without parameters, training the encoder per-\\ntains only to dense encoding methods. Different training methods\\nmay have different goals, such as improving the semantic represen-\\ntations, accelerating the encoding process, or learning the domain-\\nspecific representations. The first two goals are often achieved by\\nreplacing the original encoder with a more powerful or tiny en-\\ncoder, such as DistilBERT [142], or TinyBERT [81]. The last requires\\ntraining the original encoder on the domain-specific corpus. RE-\\nPLUG [146] updated the retriever by minimizing KL divergence\\nbetween retrieval and LM scores, and asynchronously refreshing\\nthe datastore index. After training the retriever encoder, the vector\\ndatabase’s embeddings that serve as keys will also change. Thus,\\nall indexes should be rebuilt with new embeddings. Besides, if the\\nencoder remains unchanged, the indexing can be updated using\\nnew ANN searching algorithms or re-tuning the hyperparameters.\\nAfter the retriever is trained, it can be directly incorporated into\\nthe RAG without updating the generator.\\n8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}, page_content='Datastore\\nInputs\\nOutputs\\nEncoder\\nIndexing\\nRetriever\\nGenerators\\nDatastore\\nInputs\\nOutputs\\nEncoder\\nIndexing\\nRetriever\\na. Training retriever. b. Training generator.\\nc. Jointly training retriever and generator.\\nRAG without Datastore Update RAG with Datastore Update\\nRetriever\\nGenerators\\nInputs\\nOutputs\\nEncoder\\nIndexing\\nRetriever\\nDatastore\\nNew Corpus\\nd. Updating datastore, then training generator.\\nNew Values\\nInsertion\\nQuery\\nUpdate\\nRetrieval \\nFusions\\nGenerators\\nRetrieval \\nFusions\\nRetrieval \\nFusions\\nforward\\nbackward\\ndatabase \\noperation\\nTrainable\\nEmbeddings\\ntrainable\\nfrozen\\nFigure 4: Different RAG training strategies with/without datastore update.\\n6.1.2 Training generator. Training the generator involves updating\\nits parameters or those in the retrieval fusion modules. Since the\\ngenerator is generally an LLM, training the LLM is a resource- and\\ntime-consuming process. Fortunately, several parameter-efficient\\nfine-tuning techniques, such as LoRA [61], are proposed to address\\nthe fine-tuning problem of LLMs. Although the parameters in the\\nretrieval fusion modules are less than those in the generator, only\\nfine-tuning those parameters may encounter some training prob-\\nlems, such as low convergence and overfitting. Jointly tuning the\\nparameters in the generator and the retrieval fusion modules is a\\nbetter way to train the generator and the retrieval fusion modules\\nif there are sufficient and powerful resources.\\n6.1.3 Jointly training the retriever and generator. Apart from inde-\\npendently training the retriever and the generator, jointly training\\nthe retriever and the generator can be another good choice for better\\nperformance on downstream tasks. The primary challenges involve\\ncomputational complexity from large-scale retrieval and marginal-\\nization over latent documents, alongside training instability due to\\npotential retrieval collapse and interdependent retriever-generator\\nfeedback loops. Additionally, sparse gradients from discrete re-\\ntrieval steps and approximation errors from fixed top-𝑘 document\\nselection limit effective end-to-end optimization. Typically, com-\\nplex indexes, such as FAISS [34], are not a suitable choice during\\nthe fine-tuning stage.\\nExisting works generally leverage the complex indexes to pre-\\nselect a small subset of nearest neighbors as candidates, then choose\\nthe final top-𝑘nearest neighbors by performing the matrix-multiplication\\noperations. REALM [53] and RAG [95] both unify retriever and lan-\\nguage modeling by treating retrieved documents as latent variables\\nand optimizing end-to-end via gradient descent. They both used\\nMaximum Inner Product Search (MIPS). RAG [ 95] only updated\\nthe encoder in the retriever, but REALM [53] also asynchronous re-\\nfreshed the indexing. Atlas [73] jointly pre-trained the retriever and\\ngenerator, using different loss functions (such as attention distilla-\\ntion and perplexity distillation) to optimize the retriever. Besides, it\\nalso asynchronous refreshed the indexing. Experimental results in\\nthese works demonstrate that joint training is an end-to-end opti-\\nmization that can lead to better coordination between the retriever\\nand the generator and improve the contextual understanding of the\\ngenerator.\\n6.2 RAG with Datastore Update\\nAs shown in Figure 4 (d), the scenario involves two stages: updating\\nthe knowledge database, then training the retriever and the genera-\\ntor. There are three cases for updating the knowledge database, i.e.,\\nupdating with trainable embeddings, updating with new values,\\nand updating with new corpus. In the first case, values generally\\nare trainable embeddings and are simultaneously/asynchronously\\nupdated with parameters in the RAG [16]. The last two cases usually\\nrefer to updating the knowledge database with up-to-date infor-\\nmation. Taking question-answer corpus as an example, updating\\nwith new values refers to updating the answer to existing questions,\\nwhile updating with new corpus refers to adding new question-\\nanswer pairs. To update the value of existing keys requires first\\nquerying the existing key-value pairs and then performing in-place\\nupdates. For a new corpus, the datastore first needs to perform\\ninsertion operations, then rebuilds or updates the indexes as new\\nkeys are added. After updating the datastore, training the retriever\\nand the generator is similar to RAG without datastore updates.\\nHowever, this training step is not always necessary, thanks to the\\nin-context learning capability of LLMs.\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10'}, page_content='7 RAG EVALUATION AND BENCHMARK\\nRetrieval-Augmented Generation (RAG) systems integrate the capa-\\nbilities of large language models (LLMs) with external knowledge\\nretrieval to advance text generation tasks in natural language pro-\\ncessing (NLP). Evaluating RAG systems requires a dual focus on\\nretrieval quality and generation performance to ensure the system\\neffectively leverages external knowledge and produces accurate,\\ncontextually appropriate outputs. Recent research has introduced\\ndomain-specific benchmarks and comprehensive evaluation frame-\\nworks to address these challenges.\\nFor retrieval quality, Pipitone et al. [ 128] proposed a bench-\\nmark tailored for RAG systems in the legal domain, employing\\nkey retrieval metrics such as Precision@K and Recall@K to as-\\nsess the relevance and ranking of retrieved documents. Adlakha et\\nal. [2] expanded the evaluation scope by incorporating correctness,\\nfaithfulness, and human evaluation to assess retrieved information,\\nalongside novel token-overlap metrics for finer-grained analysis.\\nOn the generation side, Hui et al. [69] introduced a benchmark\\nsuite for RAG systems in real-world document analysis, utilizing\\nmetrics such as accuracy, F1 score, and exact match score to evaluate\\nanswer quality. Similarly, Xiong et al. [174] developed a medical\\nRAG benchmark to systematically compare the performance of\\nmedical RAG systems, emphasizing domain-specific evaluation\\ncriteria.\\nRecent advancements have further enriched the evaluation land-\\nscape by proposing more systematic and comprehensive method-\\nologies. Chen et al. [ 15] identified four fundamental RAG abili-\\nties—noise robustness, negative rejection, information integration,\\nand counterfactual robustness—as key evaluation dimensions. Saad-\\nFalcon et al. [140] introduced ARES, an automated RAG evaluation\\nsystem, which assesses RAG systems using context relevance, an-\\nswer faithfulness, and answer relevance. Building on this, Friel et\\nal. [41] extended the evaluation framework by incorporating addi-\\ntional metrics such as context utilization and answer completeness,\\nproviding a more holistic assessment of RAG system performance.\\nThese efforts collectively highlight the growing emphasis on rigor-\\nous, domain-specific, and multi-dimensional evaluation methods\\nto ensure RAG systems meet the demands of diverse applications\\nwhile maintaining high retrieval and generation quality standards.\\n8 TASKS\\nThis section lists several classical tasks in the NLP domain and\\nintroduces advanced RAG techniques used to solve these tasks.\\n8.1 Language Modeling\\nLanguage modeling is the task that requires the prediction of the\\nprobability distribution of the next word or character given a se-\\nquence of words or characters, which is also named the next-token\\nprediction task. Language modeling has become the fundamental\\ntask for pre-training large language models, which can measure\\nthe models’ generation capability using the perplexity metric. The\\nformal definition is as follows: given such a sequence of tokens\\n𝑥1,...,𝑥 𝑛 called Prefix, the language modeling task aims to model\\nits probability via next-token prediction,\\n𝑝(𝑥1,...,𝑥 𝑛)= 𝑝(𝑥1)·\\n𝑛Ö\\n𝑖=2\\n𝑝(𝑥𝑖|𝑥1,...,𝑥 𝑖−1), (3)\\nwhere the conditional probabilities 𝑝(𝑥𝑖|𝑥1,...,𝑥 𝑖−1)are modeled\\nby a parameterized language model.\\nRecent works mainly leverage RAG further to improve lan-\\nguage modeling capability in the pre-training stage. A branch of\\nworks [11, 101, 163, 170] modifies the architecture of generators\\nby adding a new cross-attention module in each transformer block\\nfor introducing retrieval knowledge. The intuition of those works\\nis that given the similar Prefixes and their next tokens (retrieving\\nstage), the pre-trained model can calibrate the model’s prediction\\nusing the cross-attention module to capture the pattern between the\\nnext token and prefix (model forwarding stage). Zhong et al. [191]\\npropose to augment the language model with three types of re-\\ntrieval memories/databases (local memory, long-term memory, and\\nexternal memory) and optimize the next-token probability distribu-\\ntion with nearest neighbors retrieved from the memories/databases.\\nAnother branch of works [53, 68, 88, 133, 177] focuses on augment-\\ning the inputs or outputs of generators with retrievals. Guu et\\nal. [53] and Ram et al. [133] concatenate the retrieved knowledge\\nwith inputs and feed the retrieval-augmented inputs into the gener-\\nators. Other works [68, 88, 177] fuse the logits of inputs as well as\\nretrievals at the final output layer and generate the final probability\\ndistribution based on the interpolated results. Those works believe\\nthat the concatenated/fused retrievals can provide useful context\\ninformation on inputs/outputs to improve models’ robustness dur-\\ning the pre-training stage. Besides, Doostmohammadi et al. [ 33]\\nfocus on pre-training models with a semantic retriever (BM25) and\\nachieve a better language modeling performance.\\n8.2 Machine Translation\\nMachine translation (MT) leverages computational linguistics al-\\ngorithms to translate text or speech from one language to another\\nautomatically. The goal of MT is to produce an accurate and fluent\\ntranslation, preserving the meaning of the original text while adher-\\ning to the grammatical and stylistic norms of the target language.\\nMT systems have evolved from rule-based machine translation\\n(RBMT) to statistical machine translation (SMT) and, more recently,\\nto neural machine translation (NMT). In particular, NMT methods\\nhave significantly improved translation quality by leveraging deep\\nlearning techniques, which thus will be the focus of this section.\\nRAG techniques can further enhance MT by incorporating ex-\\nternal knowledge into the translation process. The simplest way is\\nto concatenate the similar translation examples into the inputs or\\nfuse the logits of similar translation examples at the output layer.\\nFor example, some works [ 18, 161] retrieve similar translations\\naccording to the source text and concatenate corresponding target\\ntexts or pairs of source and target texts as examples into inputs.\\nOther works [60, 87, 190] feed the retrieved source text into the\\nmodels and obtain the logits of the next target tokens, then aggre-\\ngate all logits to generate the final predictions. Moreover, Jiang et\\nal. [78] and Li et al. [ 97] use the logits of retrieved examples to\\ncalibrate the aggregated logits, improving the robustness of the\\ngeneration. Another branch of works [ 191, 192] injects external\\n10'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11'}, page_content='knowledge into the objective function during the training stage,\\nrefining the representation space with similar translations. Besides,\\nCai et al. [13] encode similar translations and store them as the trans-\\nlation memory, then introduce the knowledge from memory with\\na cross-attention module. Instead of improving the performance, a\\nbranch of work focuses on accelerating the generation efficiency\\non MT tasks, such as searching from a pre-built subset [29, 116] or\\na dynamic datastore [24], searching by chunks [113].\\n8.3 Text Summarization\\nText summarization is the process of condensing a larger text docu-\\nment into a shorter version, preserving key information and the\\noverall message. This task can be broadly categorized into two types:\\nextractive summarization, which involves selecting and compiling\\nparts of the original text, and abstractive summarization, which\\nentails rewriting the essence of the text in a new, concise form. The\\ngoal is to produce a coherent and fluent summary that encapsulates\\nthe most critical information from the source material.\\nRAG techniques can significantly enhance text summarization\\ntasks by leveraging external knowledge and similar documents to\\ninform the summarization process. [ 18, 38, 98, 161] simply con-\\ncatenates the retrieved similar summaries into inputs to generate\\nsummarizations. Instead of concatenating texts, other works fuse\\nfeatures at the intermediate layers by cross-attention [ 10], or at\\nthe output layers by logits ensemble [60]. Besides, Jiang et al. [80]\\nargue that retrieving for every generation may not always be the\\nbest choice and propose to retrieve external knowledge during the\\ngeneration process adaptively.\\n8.4 Question Answering\\nQuestion Answering (QA) is a fundamental task in NLP that in-\\nvolves building systems capable of automatically answering human\\nquestions in natural language. QA systems can be broadly classi-\\nfied into two categories: open-domain, where the system answers\\nquestions about virtually anything, and closed-domain, focusing\\non a specific area of knowledge. The primary challenge in QA is\\nunderstanding the question’s intent and retrieving accurate, rele-\\nvant information from a vast collection of data to provide a concise\\nanswer. Due to the page limits, this paper only discusses the works\\nof open-domain QA systems.\\nRAG techniques combine information retrieval with model-based\\ngeneration, which is highly suitable for QA systems. In particu-\\nlar, open-domain QA systems usually first require searching for\\nknowledge from the Internet or large-scale databases, then generate\\nthe corresponding answers according to the retrieved knowledge.\\nNaturally, given similar questions and corresponding answers as\\ndemonstrations which are concatenated into inputs [64, 98, 161],\\ngenerators in RAG can learn the pattern between questions and\\nanswers and infer what answers should be. For some specific QA\\ntasks where a set of reference documents is given, retrievers in RAG\\nwould retrieve the relevant documents for concatenation, and then\\ngenerators in RAG would read the context then generate the final\\nanswers via the self-attention mechanism [7, 53, 93, 133], which is\\nsimilar to solving a reading comprehension problem. Besides, Fabbri\\net al. [36] focus on designing effective templates for re-organizing\\nthe concatenated contexts. Baek et al. [8] leverage the knowledge\\ngraph to retrieve the related facts for the input questions, then\\nfeed their concatenation and inputs into the generators. Instead of\\ndirectly concatenating texts, another branch of works focuses on\\njoining the retrieval embeddings with input embeddings for the\\nencoder-decoder models [27, 72, 73, 141].\\nSome works incorporate the external knowledge in the hidden\\nstates or the final logits of generators. For the fusion in the hidden\\nstates, the key is what kind of knowledge representation would be\\ninjected, such as entities [28, 40], chunks [11, 158], documents [17].\\nFor the fusion in the logits, most works combine the logits of re-\\ntrievals and inputs by ensemble techniques [53, 95, 119, 146].\\nInstead of designing different knowledge fusions for QA sys-\\ntems, existing works also improve QA systems with RAG from\\nother aspects. Some works [ 48, 102, 125] use retrieved question-\\nanswering pairs as extra training data. Some works optimize the\\nretriever module, e.g., improving the keys’ representation when\\nbuilding the retriever database [132], replacing the indexing with\\na pre-trained ranking model [181], or enabling retrieving phrases\\nwith two queries [118]. Other works focus on accelerating the gen-\\neration efficiency of RAG. Jong et al. [26] propose the layer-sparse\\ncross-attention to speed up the decoding. Some works [7, 80, 165]\\nobserve that the retrievals may not always provide useful informa-\\ntion during the generation process and learn to determine when to\\nretrieve. Moreover, Sun et al. [152] combine the RAG with agents\\nto iteratively reason the final results.\\n8.5 Information Extraction\\nInformation Extraction (IE) is a critical task in NLP to automati-\\ncally extract structured information from unstructured and semi-\\nstructured text sources. This task encompasses several sub-tasks,\\nincluding Named Entity Recognition (NER), Entity Linking (EL),\\nCoreference Resolution (CR), Relation Extraction (RE), etc. The goal\\nis to identify and classify key elements from text and understand\\nthe relationships between them, thereby converting textual data\\ninto a structured format amenable to analysis and interpretation.\\nWith RAG techniques, addressing IE tasks can be significantly\\nimproved in terms of not only performance but also interpretability.\\nIn NER tasks, Wang et al. [ 164] first retrieve similar sentences\\nand then concatenate the ranked retrievals for better semantic\\nrepresentations. Ren et al. [ 138] show that naive RAG may not\\naddress Event Argument Extraction (EAE) tasks. Thus, they adopt a\\nsampling-based method to guarantee the same distribution of event\\nlabels between retrievals and inputs then concatenate retrieval texts\\ninto inputs for better performance in EAE tasks. Table augmentation\\nis also a challenging task, which requires extracting information\\nfrom tables. Glass et al. [ 45] propose to extract information in a\\nretrieval-augmented manner.\\n8.6 Text Classification\\nText classification tasks are common in NLP applications. Senti-\\nment analysis, a prominent text classification task in NLP, entails\\nidentifying and categorizing the emotional tone conveyed in a text.\\nFor example, given a sentence of “I love to watch movies”, the anal-\\nysis models should determine whether it has a positive attitude or a\\nnegative attitude. The attitude in sentiment analysis can range from\\npositive to negative or can be neutral, nuanced, and even mixed.\\n11'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12'}, page_content='The sentiment analysis task is crucial for understanding consumer\\nfeedback, monitoring brand reputation, and gaining insights into\\npublic opinion on various issues.\\nRAG techniques can significantly enhance sentiment analysis\\nwith different external knowledge fusion strategies. Li et al. [98]\\nconcatenate the retrieved options and corresponding prompt-based\\nlabels with input options. Other works [ 16, 52] concatenate the\\nretrieval embeddings with input embeddings before feeding them\\ninto the decoder. Some works fuse the retrieval features into the\\nhidden states of generators via cross-attention [17, 163] or ranking-\\nbased addition [169]. Besides, other works focus on fusing the logits\\nof retrievals with the output logit using ensemble techniques [179,\\n185]. Except for knowledge fusions, Min et al. [118] enable locating\\nknowledge in phrases more accurately via two queries.\\n8.7 Dialogue Systems\\nDialogue systems, also known as conversational agents or chatbots,\\nare designed to simulate conversation with human users, either in\\ntext or speech form. These systems can be categorized into two main\\ntypes: task-oriented systems [65], which assist users in completing\\nspecific tasks such as booking tickets or ordering food, and open-\\ndomain systems, which aim to carry on a general conversation on a\\nwide range of topics [147]. The core challenge in developing effec-\\ntive dialogue systems lies in understanding user intent, maintaining\\ncontext, and generating coherent, relevant responses.\\nExisting works improve the dialogue system with RAG mostly\\nvia the query-based fusions. Some works [ 18, 90, 100] concate-\\nnate the retrieved history conversations with current inputs. Other\\nworks [18, 39, 105] first leverage an encoder to encode the history\\nresponses, then feed the concatenated embeddings into a decoder\\nto generate new responses.\\n9 APPLICATIONS\\n9.1 LLM-based Autonomous Agents\\nLLM-based autonomous agents are intelligent software systems\\nthat leverage the power of LLMs to perform tasks without the need\\nfor continuous human intervention [ 99, 160, 171]. These agents\\nuse LLMs as a brain or controller [ 67], and extend their abilities\\nthrough multimodal perception [ 173], tool utilization [ 143] and\\nexternal memory [124]. Especially, external long-term memory for\\nagents functions as the knowledge datastore in RAG, which pro-\\nvides agents with the capability to incorporate external knowledge\\nover extended periods. Therefore, applying RAG would be bene-\\nficial to access a broader range of information, improving agents’\\ndecision-making and problem-solving abilities [187]. This section\\nexplores how LLM-based agents can leverage RAG from two per-\\nspectives.\\nUsing RAG to Retrieve from External Memory. LLM-based\\nagents can utilize RAG to access and retrieve information from their\\nown external memory [56, 114, 188]. This external memory serves\\nas a knowledge base that the agent can draw upon to enhance its\\nunderstanding and decision-making. When faced with a query or a\\ntask, the agent can use RAG to retrieve relevant information from\\nthis memory, which is then integrated into the generation process\\nof the LLM. This allows the agent to produce responses or solutions\\nthat are informed by a wider range of knowledge, leading to more\\naccurate and contextually relevant outcomes.\\nThe ability to tap into a vast external memory enables the agent\\nto continuously learn and adapt based on new information, making\\nit more effective over time. Using Tools to Search the Web and\\nRAG for Up-to-Date Information. In addition to retrieving infor-\\nmation from its own memory, an LLM-based agent can use tools to\\nsearch the web for the most current information [143]. This capabil-\\nity is particularly useful for tasks that require up-to-date knowledge,\\nsuch as news summarization, market analysis, or responding to\\nrapidly evolving situations. Once the agent retrieves the latest in-\\nformation from the web, it can use RAG to integrate this data into\\nits generation process. By combining the LLM’s natural language\\nunderstanding with real-time data from the web, the agent can\\ngenerate responses that are not only contextually relevant but also\\nreflect the latest developments. This approach enhances the agent’s\\nability to provide accurate and timely information, improving its\\neffectiveness in dynamic environments.\\nIn both cases, RAG plays a crucial role in augmenting the capabil-\\nities of LLM-based agents by enabling them to access and leverage\\na wider range of information, whether it’s from their own external\\nmemory or from real-time sources on the web. This leads to more\\ninformed decision-making and enhances the overall performance\\nof the agents.\\n9.2 Frameworks\\nFrameworks like Langchain [91] and LLaMAindex [103] pose a sig-\\nnificant impact on enhancing the practical implementation of RAG.\\nLangchain and LLaMAindex exemplify the integration of sophisti-\\ncated retrieval mechanisms with generative models, facilitating the\\nseamless incorporation of external data into the language genera-\\ntion process. This section will introduce these two representative\\nRAG frameworks in details.\\nLangchain is a framework designed to augment the capabilities\\nof language models by integrating them with external knowledge\\nsources and databases. It acts as a middleware that facilitates the\\ninteraction between language models and various data retrieval\\nsystems, enabling a more informed and accurate generation of re-\\nsponses. The core functionality of Langchain involves orchestrating\\nthe flow of information from external databases into the generative\\nprocess of language models, enhancing their ability to leverage\\ncontext and specific knowledge in their responses. This integration\\nplays a crucial role in enabling language models to perform tasks\\nthat require access to up-to-date or detailed information that is not\\ncontained within the model’s initial training data.\\nLLaMAindex is a specialized data framework that focuses on or-\\nganizing and indexing vast amounts of data to improve the retrieval\\ncapabilities of language models. This framework supports efficient\\nquerying mechanisms, allowing language models to quickly access\\nrelevant information from a structured repository. LLaMAindex is\\ndesigned to be highly scalable and can handle diverse data types,\\nfrom text documents to structured databases. The indexed data\\nsupports a wide range of applications, from simple fact retrieval\\nto complex analytical tasks, making it an indispensable tool for\\nenhancing the information retrieval phase in language models.\\n12'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='Both Langchain and LLaMAindex are deeply connected to the\\nconcept of RAG. Langchain enhances RAG by providing a structured\\nway for language models to interact with external databases and\\nknowledge sources during the generation process. On the other\\nhand, LLaMAindex serves as a powerful backend for RAG systems\\nby ensuring that the retrieval process is both fast and relevant.\\nTogether, Langchain and LLaMAindex enhance the capabilities of\\nRAG by ensuring that the language models are not only generating\\ntext based on their internal knowledge but are also capable of\\npulling in external data to provide responses that are contextually\\nenriched and informationally robust.\\n10 DISCUSSION AND FUTURE DIRECTION\\nDespite the success of the RAG for natural language processing,\\nsome challenges should be considered. This paper highlights these\\nchallenges to inspire future research and provides possible future\\nresearch directions in RAG for NLP.\\n10.1 Retrieval Quality\\nThe retrieval quality largely determines the performance of the\\ngenerator. Park et al. [126] investigated imperfect retrieval’s effect\\nand demonstrated that unanswerability or contradiction of a docu-\\nment set, which frequently leads to hallucinations. However, Ren\\net al. [137] demonstrated that the improvement brought about by\\nthe increase in the number of supporting documents is not entirely\\ndue to the increase in recall. To this end, using various evalua-\\ntion methods referred to in Section 7 for retrieval information will\\nbe more beneficial to model generation. The retrieval quality in-\\nvolves the following four key factors that must be designed. The\\nfirst consideration is determining the optimal key to use in the\\nvector database. This process typically involves subjective decision-\\nmaking and requires human effort to design effectively. The naive\\nidea is to choose inputs for the given tasks, treating each task as a\\nQA problem.\\nThe second is the choice of embedding model . After deter-\\nmining the key, the next step is leveraging embedding models to\\nconvert text into vector representations. Models such as BERT [31],\\nRoBERTa [107], or domain-specific embeddings can be crucial to\\ndetermine how well nuances and contextual meanings are cap-\\ntured. Adapting the embedding model to suit specific data types\\nor queries better can significantly enhance retrieval quality. This\\nrequires training the model on domain-specific corpora, including\\nthe types of queries and documents the system will encounter.\\nThirdly, designing effective similarity metrics is also crucial\\nto improve retrieval quality. The goal of similarity metrics is to\\nmeasure the relevance between the query and the retrieved infor-\\nmation. Some classical similarity metrics, such as cosine similarity\\nor Euclidean distance, used for ranking in the recommender sys-\\ntem can also be used in RAG [47]. Apart from these metrics, some\\nworks explored more complex similarity metrics, such as optimal\\ntransport distance [23], to obtain a task-specific similarity.\\nFinally, approximate nearest neighbor (ANN) searching is\\nalso a key step in determining what knowledge should be returned\\nas nearest neighbors. Advanced ANN searching aims to acceler-\\nate the retrieval efficiency at the cost of sacrificing the retrieval\\nquality. Choosing a suitable ANN algorithm, such as product quan-\\ntization [74] or HNSW [ 111], requires a good trade-off between\\nretrieval efficiency and retrieval quality. All of these factors collec-\\ntively contribute to the retrieval quality of the retriever.\\n10.2 RAG Efficiency\\nRAG efficiency is crucial for downstream NLP applications, which\\nlimits the volume of data that can be retrieved. There are two simple\\nways to guarantee RAG efficiency without new algorithms, i.e., re-\\nducing the volume of data or adding more powerful computing and\\nmemory resources. However, the former may impact the retrieval\\nquality, while the latter requires more resource cost.\\nRAG efficiency encompasses the efficiency of the retriever and\\nthe efficiency of retrieval fusions. Retriever efficiency refers to the\\ntime cost of retrieving relevant information, which can be divided\\ninto three parts, i.e., encoding time, ANN searching time, and data\\nfetching time of the datastore. It is unnecessary to jointly optimize\\nall three components as the bottleneck would vary from different\\ndatabase sizes. For smaller retrieval databases, such as those with\\nfewer than 1 million entries, the encoding phase is often the primary\\nbottleneck, as the vector database can be all stored in the memory.\\nSeveral topics, such as model quantization [9, 89], distillation [32,\\n81], or model pruning [42], are used to accelerate the encoding.\\nIn contrast, for larger databases, the time cost of searching in\\nthe index and fetching data from the datastore becomes the major\\nbottleneck, as the searching is over a considerable amount of data,\\nand the fetching involves I/O overheads. In this case, efficient ANN\\nsearching algorithms [34, 51, 84] and system-level optimizations [79,\\n82] are the main focus.\\nRetrieval fusion efficiency, which aims to enhance the inference\\nefficiency when integrating retrievals, is worth to be optimized\\nfor improving the RAG efficiency. For example, the computational\\noverhead of query-based fusion is often non-negligible due to the\\nlong sequence length. Some works, such as Fid-light [59] and Re-\\nFusion [169], mainly target reducing the computations while inte-\\ngrating the retrieved information.\\n10.3 Discussion for Query-based Fusion in RAG\\nTwo main query-based fusion RAG concerns must be discussed. The\\nfirst is the performance comparison towards long-context LLMs.\\nHui et al. [ 69] demonstrated that RAG system and long-context\\nLLMs show similar performance for free-form or knowledge-based\\ntasks (e.g., paper-based and wiki-based Q&A). However, in tasks\\nrequiring numerical reasoning (e.g., financial Q&A), long-context\\nLLMs underperform compared to RAG, as the verbose content in\\nlong contexts can obscure key facts and hinder precise reasoning.\\nAdditionally, RAG tends to outperform long-context mechanisms\\nin smaller LLMs, which struggle to process large volumes of data\\neffectively. The second concern is improving the query-based fu-\\nsion method, which is prevalent in modern RAG systems. Recent\\nworks used prompt engineering strategies [ 52, 95], query refine-\\nment techniques [178], calibration-based strategies [7, 80], and it-\\nerative RAG [145] to improve the query-based fusion RAG. Among\\nthese strategies, prompt engineering and calibration offer improved\\nresults with minimal overhead, while iterative retrieval-augmented\\n13'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14'}, page_content='generation (RAG) and query refinement prioritize higher computa-\\ntional costs in exchange for potential accuracy gains. Prompt engi-\\nneering and query refinement depend on manual tuning, whereas\\ncalibration and iterative RAG are more automation-friendly but\\nrequire high-quality, robust data. Simpler strategies, such as prompt-\\ning, are easier to deploy but lack flexibility, while more complex\\nmethods like iterative RAG provide greater adaptability at the ex-\\npense of stability. These approaches can be combined (e.g., refined\\nqueries with iterative loops and calibrated prompts), though such\\nsynergies must carefully balance the compounded drawbacks, such\\nas increased latency and complexity.\\n10.4 Choices of Fusions\\nThis paper introduces three kinds of retrieval fusions, where each\\nfusion is worth further exploring. Query-based fusions concatenate\\nthe texts or embeddings of retrieved knowledge with inputs. These\\nmethods have better interpretability and are easy to apply even only\\nwhen the API of LLMs is provided. However, concatenation leads to\\na long sequence of inputs, thus resulting in a large computational\\noverhead in the attention and truncation of inputs. Some works [6,\\n169] aim to improve efficiency when integrating retrievals, while\\nothers [10, 163] focus on improving the efficiency when increasing\\nthe model input length.\\nConversely, latent-based fusions amalgamate information at a\\ndeeper, more abstract level, which may capture more nuanced rela-\\ntionships between the retrieved information and the query. How-\\never, these fusions significantly lack interpretability and often re-\\nquire pre-training or fine-tuning to adjust the retrieval embeddings\\nor reweight the retrievals. Therefore, enhancing the interpretability\\nof such latent-based fusions is also worth exploring in the future.\\nLogits-based fusions incorporate information at the decision\\nlevel, thereby offering a potentially more flexible and robust inte-\\ngration of data from various sources. Nonetheless, these fusions\\nmay oversimplify the fusion process, diminishing the richness of\\nthe retrieved information by reducing them to logit values. Mean-\\nwhile, such fusions require performing all inference of retrievals,\\nwhich is also a time-consuming process.\\nApart from applying one kind of fusion in practical applications,\\ncombining different fusions is also worth exploring for better per-\\nformance. These fusion methods are not mutually exclusive, as they\\nfocus on augmenting the different stages of generators, i.e., inputs,\\nhidden states, and outputs. Besides, during the generation, when to\\nfuse retrieved knowledge is also a significant problem worthy of\\nfurther exploration [112].\\n10.5 RAG Training\\nAs introduced in Section 6, RAG training includes two branches\\nof works, RAG with/without datastore update. For RAG without a\\ndatastore update, the main challenge is how to jointly optimize all\\nparameters in RAG. This may involve new loss functions with mul-\\ntiple objectives, new optimizations for efficient tuning parameters\\nin the retriever and generator, or other training strategies.\\nFor RAG with datastore update, one challenge is how to align\\nthe retrieval representations with the generator’s representations.\\nAlthough the time cost of the update operation in datastore cannot\\nbe ignored, some works [16] reduce the update frequency by asyn-\\nchronously updating, thus achieving the alignment of knowledge\\nrepresentation and model’s representation. Another challenge is\\nwhen to retrain/fine-tune the generator in RAG when a new cor-\\npus is added. Due to the in-context learning capability of existing\\nLLM-based generators and high training overhead, retraining/fine-\\ntuning the generator or directly inferring the generator becomes a\\nchallenging choice for different scenarios. Recently, some efficient\\ntraining strategies [30, 61] have been proposed to accelerate the\\nfine-tuning process, which can be taken into consideration.\\n10.6 Cross-Modality Retrieval\\nRetrieving cross-modality information in NLP tasks can greatly\\nenhance the quality and richness of the representations, leading\\nto improved performance. First, cross-modality information, such\\nas combining text with images, videos, or audio, provides a richer\\ncontext to the content [62]. For instance, when language is ambigu-\\nous, accompanying images can clarify meanings difficult to convey\\nthrough text alone. Second, different modalities can contribute vari-\\nous types of information that are not accessible from a single source.\\nFor example, visual data can provide spatial, color, and action cues,\\nwhile textual data can offer detailed descriptions, emotions, or\\nabstract concepts. Combining these can lead to a more compre-\\nhensive understanding of the data. Moreover, Models trained on\\nmulti-modal data typically exhibit increased robustness and gener-\\nalizability [168]. These models are adept at associating information\\nacross diverse inputs, mitigating overfitting to the peculiarities of a\\nsingle modality. This attribute is particularly valuable in real-world\\napplications of NLP, such as in autonomous vehicles, where sys-\\ntems must interpret textual information from signs or dialogues and\\nsensory data from the surrounding environment to make informed\\ndecisions. Furthermore, multi-modal data can resolve ambiguities\\nthat cannot be resolved within a single modality. For example, the\\nphrase \"bank\" can refer to either a financial institution or the side of\\na river, and visual context can help disambiguate this. Last, human\\ncommunication is inherently multi-modal, incorporating elements\\nsuch as gestures, facial expressions, and tone of voice. Systems ca-\\npable of processing multiple modes of communication can interact\\nwith humans in a manner that is both more natural and intuitive. In\\nconclusion, integrating cross-modality information in RAG for NLP\\ntasks not only enhances the richness and quality of data representa-\\ntions but also significantly improves the systems’ comprehension,\\ninteraction capabilities, and adaptability to diverse applications.\\n11 CONCLUSION\\nIn this survey, we delve into the development of RAG within the\\nfield of natural language processing. First, this paper introduces\\nthe components of RAG and their functionalities. Subsequently,\\nthis paper elaborates on each step involved in retriever, discussing\\nthe diverse techniques. Furthermore, this paper categorizes the\\nretrieval fusions, evaluating the strengths and weaknesses inherent\\nin each retrieval fusion technique. Besides, this paper discusses the\\nRAG training, including RAG with/without datastore update. Then,\\nthis paper presents RAG evaluation and benchmarking, and ex-\\nplores how RAG can be adapted for various NLP tasks and provides\\npractical applications of RAG in real-world scenarios. Conclusively,\\n14'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15'}, page_content='this paper identifies ongoing challenges and suggests directions for\\nfuture research to foster advancements in this evolving area.\\nREFERENCES\\n[1] Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. 2022.\\nExploring the Limits of Large Scale Pre-training. In The Tenth International\\nConference on Learning Representations (ICLR) .\\n[2] Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade,\\nand Siva Reddy. 2024. Evaluating Correctness and Faithfulness of Instruction-\\nFollowing Models for Question Answering. Trans. Assoc. Comput. Linguistics\\n12 (2024), 681–699.\\n[3] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico\\nLebrón, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query\\nTransformer Models from Multi-Head Checkpoints. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Language Processing (EMNLP) ,\\nHouda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computa-\\ntional Linguistics, 4895–4901.\\n[4] Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona T. Diab, and Marjan\\nGhazvininejad. 2022. A Review on Language Models as Knowledge Bases.CoRR\\nabs/2204.06031 (2022).\\n[5] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui\\nYu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-\\nlican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian\\nSchrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, An-\\ngeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham,\\nTom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu,\\nRyan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira,\\nKareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun,\\nIain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders\\nAndreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gon-\\nzalez, Misha Khalman, Jakub Sygnowski, and et al. 2023. Gemini: A Family of\\nHighly Capable Multimodal Models. CoRR abs/2312.11805 (2023).\\n[6] Md. Adnan Arefeen, Biplob Debnath, and Srimat Chakradhar. 2023. LeanCon-\\ntext: Cost-Efficient Domain-Specific Question Answering Using LLMs. CoRR\\nabs/2309.00841 (2023).\\n[7] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.\\n2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-\\nReflection. In The Twelfth International Conference on Learning Representations,\\nICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net.\\n[8] Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-Augmented\\nLanguage Model Prompting for Zero-Shot Knowledge Graph Question Answer-\\ning. CoRR abs/2306.04136 (2023).\\n[9] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu,\\nMichael R. Lyu, and Irwin King. 2021. BinaryBERT: Pushing the Limit of\\nBERT Quantization. In Proceedings of the 59th Annual Meeting of the Associa-\\ntion for Computational Linguistics and the 11th International Joint Conference\\non Natural Language Processing (ACL/IJCNLP) . Association for Computational\\nLinguistics, 4334–4348.\\n[10] Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. 2023.\\nUnlimiformer: Long-Range Transformers with Unlimited Length Input. In Ad-\\nvances in Neural Information Processing Systems 36 (NeurIPS) .\\n[11] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-\\nford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman\\nRing, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cas-\\nsirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon\\nOsindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022.\\nImproving Language Models by Retrieving from Trillions of Tokens. InProceed-\\nings of the 39th International Conference on Machine Learning (ICML) (Proceedings\\nof Machine Learning Research) , Vol. 162. 2206–2240.\\n[12] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Ka-\\nplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\\nford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot\\nLearners. In Advances in Neural Information Processing Systems 33 (NeurIPS) .\\n[13] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. 2021. Neural\\nMachine Translation with Monolingual Translation Memory. In Proceedings\\nof the 59th Annual Meeting of the Association for Computational Linguistics\\nand the 11th International Joint Conference on Natural Language Processing\\n(ACL/IJCNLP). 7307–7318.\\n[14] Junying Chen, Qingcai Chen, Dongfang Li, and Yutao Huang. 2022. SeDR:\\nSegment Representation Learning for Long Documents Dense Retrieval. CoRR\\nabs/2211.10841 (2022).\\n[15] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking\\nLarge Language Models in Retrieval-Augmented Generation. In Thirty-Eighth\\nAAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium\\non Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27,\\n2024, Vancouver, Canada , Michael J. Wooldridge, Jennifer G. Dy, and Sriraam\\nNatarajan (Eds.). AAAI Press, 17754–17762.\\n[16] Xiang Chen, Lei Li, Ningyu Zhang, Xiaozhuan Liang, Shumin Deng, Chuanqi\\nTan, Fei Huang, Luo Si, and Huajun Chen. 2022. Decoupling Knowledge from\\nMemorization: Retrieval-augmented Prompt Learning. In Advances in Neural\\nInformation Processing Systems 35 (NeurIPS) .\\n[17] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. 2023.\\nDecouple knowledge from paramters for plug-and-play language modeling. In\\nFindings of the Association for Computational Linguistics (ACL) . 14288–14308.\\n[18] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan.\\n2023. Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory.\\nIn Advances in Neural Information Processing Systems 36 (NeurIPS) .\\n[19] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023.\\nAdapting Language Models to Compress Contexts. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Language Processing (EMNLP) . 3829–\\n3846.\\n[20] Yung-Sung Chuang, Wei Fang, Shang-Wen Li, Wen-tau Yih, and James R. Glass.\\n2023. Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Ques-\\ntion Answering. In Findings of the Association for Computational Linguistics\\n(ACL). Association for Computational Linguistics, 12131–12147.\\n[21] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.\\n2020. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than\\nGenerators. In 8th International Conference on Learning Representations (ICLR) .\\nOpenReview.net.\\n[22] Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo,\\nCaio Corro, André F. T. Martins, Fabrizio Esposito, Vera Lúcia Raposo, Sofia\\nMorgado, and Michael Desa. 2024. SaulLM-7B: A pioneering Large Language\\nModel for Law. CoRR abs/2403.03883 (2024).\\n[23] Yufei Cui, Ziquan Liu, Yixin Chen, Yuchen Lu, Xinyue Yu, Xue (Steve) Liu,\\nTei-Wei Kuo, Miguel Rodrigues, Chun Jason Xue, and Antoni B. Chan. 2023.\\nRetrieval-Augmented Multiple Instance Learning. In Advances in Neural Infor-\\nmation Processing Systems 36 (NeurIPS) .\\n[24] Yuhan Dai, Zhirui Zhang, Qiuzhi Liu, Qu Cui, Weihua Li, Yichao Du, and Tong\\nXu. 2023. Simple and Scalable Nearest Neighbor Machine Translation. In The\\nEleventh International Conference on Learning Representations (ICLR) .\\n[25] David Dale, Elena Voita, Loïc Barrault, and Marta R. Costa-jussà. 2023. Detecting\\nand Mitigating Hallucinations in Machine Translation: Model Internal Workings\\nAlone Do Well, Sentence Similarity Even Better. InProceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (ACL) . Association for\\nComputational Linguistics, 36–50.\\n[26] Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit\\nSanghai, Fei Sha, and William W. Cohen. 2023. FiDO: Fusion-in-Decoder opti-\\nmized for stronger performance and faster inference. In Findings of the Associa-\\ntion for Computational Linguistics (ACL) . 11534–11547.\\n[27] Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Joshua Ainslie, Sumit\\nSanghai, Fei Sha, and William W. Cohen. 2023. Pre-computed memory or on-\\nthe-fly encoding? A hybrid approach to retrieval augmentation makes the most\\nof your compute. In Proceedings of the 40th International Conference on Machine\\nLearning (ICML) . 7329–7342.\\n[28] Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William W.\\nCohen. 2022. Mention Memory: incorporating textual knowledge into Trans-\\nformers through entity mention attention. InThe Tenth International Conference\\non Learning Representations (ICLR) .\\n[29] Hiroyuki Deguchi, Taro Watanabe, Yusuke Matsui, Masao Utiyama, Hideki\\nTanaka, and Eiichiro Sumita. 2023. Subset Retrieval Nearest Neighbor Machine\\nTranslation. In Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (ACL) . 174–189.\\n[30] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.\\nQLoRA: Efficient Finetuning of Quantized LLMs. In Advances in Neural Infor-\\nmation Processing Systems 36 (NeurIPS) .\\n[31] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\\nBERT: Pre-training of Deep Bidirectional Transformers for Language Under-\\nstanding. In Proceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies\\n(NAACL-HLT). 4171–4186.\\n[32] Zixiang Ding, Guoqing Jiang, Shuai Zhang, Lin Guo, and Wei Lin. 2023. SKD-\\nBERT: Compressing BERT via Stochastic Knowledge Distillation. In Thirty-\\nSeventh AAAI Conference on Artificial Intelligence (AAAI) . AAAI Press, 7414–\\n7422.\\n[33] Ehsan Doostmohammadi, Tobias Norlund, Marco Kuhlmann, and Richard\\nJohansson. 2023. Surface-Based Retrieval Reduces Perplexity of Retrieval-\\nAugmented Language Models. In Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (ACL) . 521–529.\\n15'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16'}, page_content='[34] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,\\nPierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2024.\\nThe Faiss library. CoRR abs/2401.08281 (2024).\\n[35] explosion. 2016. Spacy. https://spacy.io/\\n[36] Alexander R. Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, and Bing\\nXiang. 2020. Template-Based Question Generation from Retrieved Sentences for\\nImproved Unsupervised Question Answering. In Proceedings of the 58th Annual\\nMeeting of the Association for Computational Linguistics (ACL) . 4508–4513.\\n[37] Facebook. 2013. RocksDB. https://github.com/facebook/rocksdb\\n[38] Angela Fan and Claire Gardent. 2022. Generating Full Length Wikipedia Bi-\\nographies: The Impact of Gender Bias on the Retrieval-Based Generation of\\nWomen Biographies. CoRR abs/2204.05879 (2022).\\n[39] Angela Fan, Claire Gardent, Chloé Braud, and Antoine Bordes. 2021. Augment-\\ning Transformers with KNN-Based Composite Memory for Dialog.Trans. Assoc.\\nComput. Linguistics 9 (2021), 82–99.\\n[40] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom\\nKwiatkowski. 2020. Entities as Experts: Sparse Memory Access with Entity\\nSupervision. In Proceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing (EMNLP) . 4937–4951.\\n[41] Robert Friel, Masha Belyi, and Atindriyo Sanyal. 2024. Ragbench: Explain-\\nable benchmark for retrieval-augmented generation systems. arXiv preprint\\narXiv:2407.11005 (2024).\\n[42] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan\\nSajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. 2021. Compressing\\nLarge-Scale Transformer-Based Models: A Case Study on BERT. Trans. Assoc.\\nComput. Linguistics 9 (2021), 1061–1080.\\n[43] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive\\nLearning of Sentence Embeddings. In Proceedings of the 2021 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP) . Association for\\nComputational Linguistics, 6894–6910.\\n[44] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,\\nYi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.\\nRetrieval-Augmented Generation for Large Language Models: A Survey. CoRR\\nabs/2312.10997 (2023).\\n[45] Michael R. Glass, Xueqing Wu, Ankita Rajaram Naik, Gaetano Rossiello, and\\nAlfio Gliozzo. 2023. Retrieval-Based Transformer for Table Augmentation. In\\nFindings of the Association for Computational Linguistics (ACL) . 5635–5648.\\n[46] Hongyu Gong, Yelong Shen, Dian Yu, Jianshu Chen, and Dong Yu. 2020. Recur-\\nrent Chunking Mechanisms for Long-Text Machine Reading Comprehension.\\nIn Proceedings of the 58th Annual Meeting of the Association for Computational\\nLinguistics (ACL). Association for Computational Linguistics, 6751–6761.\\n[47] Asela Gunawardana and Guy Shani. 2009. A Survey of Accuracy Evaluation\\nMetrics of Recommendation Tasks. J. Mach. Learn. Res. 10 (2009), 2935–2962.\\n[48] Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, and Jian Yin. 2019. Coupling\\nRetrieval and Meta-Learning for Context-Dependent Semantic Parsing. In Pro-\\nceedings of the 57th Conference of the Association for Computational Linguistics\\n(ACL). 855–866.\\n[49] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin\\nXu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al . 2025. Deepseek-r1:\\nIncentivizing reasoning capability in llms via reinforcement learning. arXiv\\npreprint arXiv:2501.12948 (2025).\\n[50] Rentong Guo, Xiaofan Luan, Long Xiang, Xiao Yan, Xiaomeng Yi, Jigao Luo,\\nQianya Cheng, Weizhi Xu, Jiarui Luo, Frank Liu, Zhenshan Cao, Yanliang Qiao,\\nTing Wang, Bo Tang, and Charles Xie. 2022. Manu: A Cloud Native Vector\\nDatabase Management System. Proc. VLDB Endow. 15, 12 (2022), 3548–3561.\\n[51] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern,\\nand Sanjiv Kumar. 2020. Accelerating Large-Scale Inference with Anisotropic\\nVector Quantization. In Proceedings of the 37th International Conference on\\nMachine Learning (ICML) (Proceedings of Machine Learning Research) , Vol. 119.\\nPMLR, 3887–3896.\\n[52] Zhicheng Guo, Sijie Cheng, Yile Wang, Peng Li, and Yang Liu. 2023. Prompt-\\nGuided Retrieval Augmentation for Non-Knowledge-Intensive Tasks. In Find-\\nings of the Association for Computational Linguistics (ACL) . 10896–10912.\\n[53] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.\\n2020. Retrieval Augmented Language Model Pre-Training. In Proceedings of\\nthe 37th International Conference on Machine Learning (ICML) (Proceedings of\\nMachine Learning Research) , Vol. 119. 3929–3938.\\n[54] David Harris and Sarah Harris. 2010. Digital design and computer architecture .\\nMorgan Kaufmann.\\n[55] Zellig S Harris. 1954. Distributional structure. Word 10, 2-3 (1954), 146–162.\\n[56] Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert,\\nAdam Amos-Binks, Zohreh Dannenhauer, and Dustin Dannenhauer. 2023. Mem-\\nory Matters: The Need to Improve Long-Term Memory in LLM-Agents. In\\nProceedings of the AAAI Symposium Series , Vol. 2. 277–280.\\n[57] Qiyuan He, Yizhong Wang, and Wenya Wang. 2024. Can Language Models Act\\nas Knowledge Bases at Scale? CoRR abs/2402.14273 (2024).\\n[58] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den\\nDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\\nErich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training\\nCompute-Optimal Large Language Models. CoRR abs/2203.15556 (2022).\\n[59] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023.\\nFiD-Light: Efficient and Effective Retrieval-Augmented Text Generation. In\\nProceedings of the 46th International ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval (SIGIR) . ACM, 1437–1447.\\n[60] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. 2020. Simple\\nand Effective Retrieve-Edit-Rerank Text Generation. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Linguistics (ACL) . 2532–\\n2538.\\n[61] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\nWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of\\nLarge Language Models. In The Tenth International Conference on Learning\\nRepresentations (ICLR).\\n[62] Xuming Hu. 2023. Multimodal Named Entity Recognition and Relation Extrac-\\ntion with Retrieval-Augmented Strategy. InProceedings of the 46th International\\nACM SIGIR Conference on Research and Development in Information Retrieval\\n(SIGIR). ACM, 3488.\\n[63] Yucheng Hu and Yuxing Lu. 2024. RAG and RAU: A Survey on Retrieval-\\nAugmented Language Model in Natural Language Processing. CoRR\\nabs/2404.19543 (2024).\\n[64] Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang,\\nand Bryan Catanzaro. 2023. RAVEN: In-Context Learning with Retrieval Aug-\\nmented Encoder-Decoder Language Models. CoRR abs/2308.07922 (2023).\\n[65] Qiushi Huang, Shuai Fu, Xubo Liu, Wenwu Wang, Tom Ko, Yu Zhang, and Lilian\\nH. Y. Tang. 2023. Learning Retrieval Augmentation for Personalized Dialogue\\nGeneration. In Proceedings of the 2023 Conference on Empirical Methods in Natu-\\nral Language Processing (EMNLP) . Association for Computational Linguistics,\\n2523–2540.\\n[66] Qiang Huang and Anthony K. H. Tung. 2023. Lightweight-Yet-Efficient: Revi-\\ntalizing Ball-Tree for Point-to-Hyperplane Nearest Neighbor Search. In 39th\\nIEEE International Conference on Data Engineering (ICDE) . IEEE, 436–449.\\n[67] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian,\\nYasheng Wang, Ruiming Tang, and Enhong Chen. 2024. Understanding the\\nplanning of LLM agents: A survey. CoRR abs/2402.02716 (2024).\\n[68] Yangsibo Huang, Daogao Liu, Zexuan Zhong, Weijia Shi, and Yin Tat Lee. 2023.\\nkNN-Adapter: Efficient Domain Adaptation for Black-Box Language Models.\\nCoRR abs/2302.10879 (2023).\\n[69] Yulong Hui, Yao Lu, and Huanchen Zhang. 2024. UDA: A Benchmark Suite\\nfor Retrieval Augmented Generation in Real-World Document Analysis. In\\nAdvances in Neural Information Processing Systems 38: Annual Conference on\\nNeural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada,\\nDecember 10 - 15, 2024 , Amir Globersons, Lester Mackey, Danielle Belgrave,\\nAngela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.).\\n[70] Shonosuke Ishiwatari, Jingtao Yao, Shujie Liu, Mu Li, Ming Zhou, Naoki Yoshi-\\nnaga, Masaru Kitsuregawa, and Weijia Jia. 2017. Chunk-based Decoder for\\nNeural Machine Translation. In Proceedings of the 55th Annual Meeting of the\\nAssociation for Computational Linguistics (ACL) . Association for Computational\\nLinguistics, 1901–1912.\\n[71] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense\\nInformation Retrieval with Contrastive Learning. Trans. Mach. Learn. Res. 2022\\n(2022).\\n[72] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with\\nGenerative Models for Open Domain Question Answering. InProceedings of the\\n16th Conference of the European Chapter of the Association for Computational\\nLinguistics (EACL). 874–880.\\n[73] Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,\\nTimo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard\\nGrave. 2023. Atlas: Few-shot Learning with Retrieval Augmented Language\\nModels. J. Mach. Learn. Res. 24 (2023), 251:1–251:43.\\n[74] Hervé Jégou, Matthijs Douze, and Cordelia Schmid. 2011. Product Quantization\\nfor Nearest Neighbor Search. IEEE Trans. Pattern Anal. Mach. Intell. 33, 1 (2011),\\n117–128.\\n[75] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\\nYejin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in\\nNatural Language Generation. ACM Comput. Surv. 55, 12 (2023), 248:1–248:38.\\n[76] Ziwei Ji, Zihan Liu, Nayeon Lee, Tiezheng Yu, Bryan Wilie, Min Zeng, and\\nPascale Fung. 2023. RHO: Reducing Hallucination in Open-domain Dialogues\\nwith Knowledge Grounding. In Findings of the Association for Computational\\nLinguistics (ACL). Association for Computational Linguistics, 4504–4522.\\n[77] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-\\nvendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nand William El Sayed. 2023. Mistral 7B. CoRR abs/2310.06825 (2023).\\n16'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17'}, page_content='[78] Hui Jiang, Ziyao Lu, Fandong Meng, Chulun Zhou, Jie Zhou, Degen Huang, and\\nJinsong Su. 2022. Towards Robust k-Nearest-Neighbor Machine Translation. In\\nProceedings of the 2022 Conference on Empirical Methods in Natural Language\\nProcessing (EMNLP) . 5468–5477.\\n[79] Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, and Tim Kraska.\\n2024. PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System\\nCo-design. CoRR abs/2403.05676 (2024).\\n[80] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-\\nYu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval\\nAugmented Generation. In Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) . 7969–7992.\\n[81] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang\\nWang, and Qun Liu. 2020. TinyBERT: Distilling BERT for Natural Language\\nUnderstanding. In Findings of the Association for Computational Linguistics\\n(EMNLP) (Findings of ACL) , Vol. EMNLP 2020. Association for Computational\\nLinguistics, 4163–4174.\\n[82] Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin\\nJin. 2024. RAGCache: Efficient Knowledge Caching for Retrieval-Augmented\\nGeneration. CoRR abs/2404.12457 (2024).\\n[83] Qiao Jin, Won Kim, Qingyu Chen, Donald C. Comeau, Lana Yeganova, W. John\\nWilbur, and Zhiyong Lu. 2023. MedCPT: Contrastive Pre-trained Transformers\\nwith large-scale PubMed search logs for zero-shot biomedical information\\nretrieval. Bioinform. 39, 10 (2023).\\n[84] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021. Billion-Scale Similarity\\nSearch with GPUs. IEEE Trans. Big Data 7, 3 (2021), 535–547.\\n[85] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,\\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\\nScaling Laws for Neural Language Models. CoRR abs/2001.08361 (2020).\\n[86] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu,\\nSergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval\\nfor Open-Domain Question Answering. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing (EMNLP) . 6769–6781.\\n[87] Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike\\nLewis. 2020. Nearest Neighbor Machine Translation. CoRR abs/2010.00710\\n(2020).\\n[88] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike\\nLewis. 2020. Generalization through Memorization: Nearest Neighbor Language\\nModels. In The 8th International Conference on Learning Representations (ICLR) .\\n[89] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer.\\n2021. I-BERT: Integer-only BERT Quantization. In Proceedings of the 38th\\nInternational Conference on Machine Learning (ICML) (Proceedings of Machine\\nLearning Research), Vol. 139. PMLR, 5506–5518.\\n[90] Brendan King and Jeffrey Flanigan. 2023. Diverse Retrieval-Augmented In-\\nContext Learning for Dialogue State Tracking. In Findings of the Association for\\nComputational Linguistics (ACL) . 5570–5585.\\n[91] LangChain. 2023. LangChain. https://www.langchain.com/\\n[92] Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grig-\\norev. 2022. Internet-augmented language models through few-shot prompting\\nfor open-domain question answering. CoRR abs/2203.05115 (2022).\\n[93] Kyungjae Lee, Sang-eun Han, Seung-won Hwang, and Moontae Lee. 2023. When\\nto Read Documents or QA History: On Unified and Selective Open-domain QA.\\nIn Findings of the Association for Computational Linguistics (ACL) . 6420–6432.\\n[94] Patrick S. H. Lewis, Barlas Oguz, Wenhan Xiong, Fabio Petroni, Scott Yih, and\\nSebastian Riedel. 2022. Boosted Dense Retriever. In Proceedings of the 2022\\nConference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United\\nStates, July 10-15, 2022 , Marine Carpuat, Marie-Catherine de Marneffe, and\\nIván Vladimir Meza Ruíz (Eds.). Association for Computational Linguistics,\\n3102–3117.\\n[95] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\\nRocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented\\nGeneration for Knowledge-Intensive NLP Tasks. In Advances in Neural Infor-\\nmation Processing Systems 33 (NeurIPS) .\\n[96] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A Survey\\non Retrieval-Augmented Text Generation. CoRR abs/2202.01110 (2022).\\n[97] Xuanhong Li, Peng Li, and Po Hu. 2023. Revisiting Source Context in Near-\\nest Neighbor Machine Translation. In Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP) .\\n[98] Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie,\\nXiaoling Wang, and Xipeng Qiu. 2023. Unified Demonstration Retriever for\\nIn-Context Learning. In Proceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (ACL) . 4644–4668.\\n[99] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu,\\nJiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei\\nGeng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang Li,\\nMengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, and Yunxin Liu. 2024.\\nPersonal LLM Agents: Insights and Survey about the Capability, Efficiency and\\nSecurity. CoRR abs/2401.05459 (2024).\\n[100] Yunhao Li, Yunyi Yang, Xiaojun Quan, and Jianxing Yu. 2021. Retrieve &\\nMemorize: Dialog Policy Learning with Multi-Action Memory. InFindings of\\nthe Association for Computational Linguistics (ACL/IJCNLP) . 447–459.\\n[101] Zonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022. Decoupled Context Processing\\nfor Context Augmented Language Modeling. In Advances in Neural Information\\nProcessing Systems 35 (NeurIPS) .\\n[102] Bill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen Tian, and Xiang Ren.\\n2022. Unsupervised Cross-Task Generalization via Retrieval Augmentation. In\\nAdvances in Neural Information Processing Systems 35 (NeurIPS) .\\n[103] Jerry Liu. 2022. LlamaIndex. https://doi.org/10.5281/zenodo.1234\\n[104] Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian. 2023.\\nTCRA-LLM: Token Compression Retrieval Augmented Large Language Model\\nfor Inference Cost Reduction. In Findings of the Association for Computational\\nLinguistics (EMNLP) . 9796–9810.\\n[105] Shuai Liu, Hyundong Cho, Marjorie Freedman, Xuezhe Ma, and Jonathan May.\\n2023. RECAP: Retrieval-Enhanced Context-Aware Prefix Encoder for Personal-\\nized Dialogue Response Generation. In Proceedings of the 61st Annual Meeting\\nof the Association for Computational Linguistics (ACL) . 8404–8419.\\n[106] Shi-guang Liu and Yin-wei Wei. 2015. Fast nearest neighbor searching based\\non improved VP-tree. Pattern Recognit. Lett. 60-61 (2015), 8–15.\\n[107] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR\\nabs/1907.11692 (2019).\\n[108] LMDB. 2014. LMDB. https://github.com/LMDB/lmdb\\n[109] Man Luo, Shashank Jain, Anchit Gupta, Arash Einolghozati, Barlas Oguz, Debo-\\njeet Chatterjee, Xilun Chen, Chitta Baral, and Peyman Heidari. 2023. A Study on\\nthe Efficiency and Generalization of Light Hybrid Retrievers. In Proceedings of\\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume\\n2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jor-\\ndan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational\\nLinguistics, 1617–1626.\\n[110] Xiaozhong Lyu, Stefan Grafberger, Samantha Biegel, Shaopeng Wei, Meng Cao,\\nSebastian Schelter, and Ce Zhang. 2023. Improving Retrieval-Augmented Large\\nLanguage Models via Data Importance Learning. CoRR abs/2307.03027 (2023).\\n[111] Yury A. Malkov and Dmitry A. Yashunin. 2020. Efficient and Robust Approx-\\nimate Nearest Neighbor Search Using Hierarchical Navigable Small World\\nGraphs. IEEE Trans. Pattern Anal. Mach. Intell. 42, 4 (2020), 824–836.\\n[112] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and\\nHannaneh Hajishirzi. 2023. When Not to Trust Language Models: Investigating\\nEffectiveness of Parametric and Non-Parametric Memories. In Proceedings of\\nthe 61st Annual Meeting of the Association for Computational Linguistics (ACL) .\\n9802–9822.\\n[113] Pedro Henrique Martins, Zita Marinho, and André F. T. Martins. 2022. Chunk-\\nbased Nearest Neighbor Machine Translation. In Proceedings of the 2022 Confer-\\nence on Empirical Methods in Natural Language Processing (EMNLP) . 4228–4245.\\n[114] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng\\nZhang. 2024. AIOS: LLM Agent Operating System. CoRR abs/2403.16971 (2024).\\n[115] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating\\nand Editing Factual Associations in GPT. In Advances in Neural Information\\nProcessing Systems 35 (NeurIPS) .\\n[116] Yuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xiaofei Sun, Tianwei Zhang,\\nand Jiwei Li. 2022. Fast Nearest Neighbor Machine Translation. In Findings of\\nthe Association for Computational Linguistics (ACL) . 555–565.\\n[117] Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya\\nPathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya\\nTafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua,\\nAlex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti,\\nAnna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan,\\nChristopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito,\\nDavid Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker,\\nGeorge-Cristian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian\\nTenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-\\nBaptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret,\\nJustin Chiu, and et al. 2024. Gemma: Open Models Based on Gemini Research\\nand Technology. CoRR abs/2403.08295 (2024).\\n[118] Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh\\nHajishirzi, and Luke Zettlemoyer. 2023. Nonparametric Masked Language\\nModeling. In Findings of the Association for Computational Linguistics (ACL) .\\n2097–2118.\\n[119] Aaron Mueller, Kanika Narang, Lambert Mathias, Qifan Wang, and Hamed\\nFirooz. 2023. Meta-training with Demonstration Retrieval for Efficient Few-\\nshot Learning. In Findings of the Association for Computational Linguistics (ACL) .\\n6049–6064.\\n[120] Ewa Muszynska. 2016. Graph- and surface-level sentence chunking. In Proceed-\\nings of the ACL 2016 Student Research Workshop . Association for Computational\\nLinguistics, 93–99.\\n17'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18'}, page_content='[121] NLTK. 2001. NLTK. https://www.nltk.org/\\n[122] OpenAI. 2022. Text-Emb-Ada. https://platform.openai.com/docs/guides/\\nembeddings\\n[123] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023).\\n[124] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and\\nJoseph E. Gonzalez. 2023. MemGPT: Towards LLMs as Operating Systems.\\nCoRR abs/2310.08560 (2023).\\n[125] Bhargavi Paranjape, Matthew Lamm, and Ian Tenney. 2022. Retrieval-guided\\nCounterfactual Generation for QA. In Proceedings of the 60th Annual Meeting of\\nthe Association for Computational Linguistics (ACL) . 1670–1686.\\n[126] Seong-Il Park and Jay-Yoon Lee. 2024. Toward Robust RALMs: Revealing the\\nImpact of Imperfect Retrieval on Retrieval-Augmented Language Models.Trans.\\nAssoc. Comput. Linguistics 12 (2024), 1686–1702.\\n[127] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton\\nBakhtin, Yuxiang Wu, and Alexander H. Miller. 2019. Language Models as\\nKnowledge Bases?. In Proceedings of the 2019 Conference on Empirical Methods\\nin Natural Language Processing and the 9th International Joint Conference on\\nNatural Language Processing (EMNLP-IJCNLP) , Kentaro Inui, Jing Jiang, Vincent\\nNg, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 2463–\\n2473.\\n[128] Nicholas Pipitone and Ghita Houir Alami. 2024. LegalBench-RAG: A Benchmark\\nfor Retrieval-Augmented Generation in the Legal Domain.CoRR abs/2408.10343\\n(2024).\\n[129] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.\\nImproving language understanding by generative pre-training. OpenAI blog\\n(2018).\\n[130] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\\nSutskever. 2019. Language models are unsupervised multitask learners. OpenAI\\nblog 1, 8 (2019), 9.\\n[131] Anand Rajaraman and Jeffrey David Ullman. 2011. Data Mining . Cambridge\\nUniversity Press, 1–17.\\n[132] Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov, Jonathan Berant, and Amir\\nGloberson. 2023. What Are You Token About? Dense Retrieval as Distributions\\nOver the Vocabulary. InProceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (ACL) . 2481–2498.\\n[133] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin\\nLeyton-Brown, and Yoav Shoham. 2023. In-Context Retrieval-Augmented Lan-\\nguage Models. Trans. Assoc. Comput. Linguistics 11 (2023), 1316–1331.\\n[134] Parikshit Ram and Kaushik Sinha. 2019. Revisiting kd-tree for Nearest Neighbor\\nSearch. In Proceedings of the 25th ACM SIGKDD International Conference on\\nKnowledge Discovery & Data Mining (KDD) , Ankur Teredesai, Vipin Kumar, Ying\\nLi, Rómer Rosales, Evimaria Terzi, and George Karypis (Eds.). ACM, 1378–1388.\\n[135] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P.\\nLillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-\\nrat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud,\\nAndrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux,\\nBenjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy,\\nJilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin\\nJohnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Ka-\\nreem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang,\\nHenryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard\\nIves, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam,\\nAakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et\\nal. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of\\ntokens of context. CoRR abs/2403.05530 (2024).\\n[136] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\\nusing Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-\\npirical Methods in Natural Language Processing and the 9th International Joint\\nConference on Natural Language Processing (EMNLP-IJCNLP) . 3980–3990.\\n[137] Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hua Wu,\\nJi-Rong Wen, and Haifeng Wang. 2025. Investigating the Factual Knowledge\\nBoundary of Large Language Models with Retrieval Augmentation. In Proceed-\\nings of the 31st International Conference on Computational Linguistics, COLING\\n2025, Abu Dhabi, UAE, January 19-24, 2025 , Owen Rambow, Leo Wanner, Mari-\\nanna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert\\n(Eds.). Association for Computational Linguistics, 3697–3715.\\n[138] Yubing Ren, Yanan Cao, Ping Guo, Fang Fang, Wei Ma, and Zheng Lin. 2023.\\nRetrieve-and-Sample: Document-level Event Argument Extraction via Hybrid\\nRetrieval Augmentation. In Proceedings of the 61st Annual Meeting of the Associ-\\nation for Computational Linguistics (ACL) . 293–306.\\n[139] Stephen E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance\\nFramework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (2009), 333–389.\\n[140] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2024.\\nARES: An Automated Evaluation Framework for Retrieval-Augmented Gen-\\neration Systems. In Proceedings of the 2024 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21,\\n2024, Kevin Duh, Helena Gómez-Adorno, and Steven Bethard (Eds.). Association\\nfor Computational Linguistics, 338–354.\\n[141] Devendra Singh Sachan, Siva Reddy, William L. Hamilton, Chris Dyer, and Dani\\nYogatama. 2021. End-to-End Training of Multi-Document Reader and Retriever\\nfor Open-Domain Question Answering. In Advances in Neural Information\\nProcessing Systems 34 (NeurIPS) . 25968–25981.\\n[142] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distil-\\nBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR\\nabs/1910.01108 (2019).\\n[143] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,\\nEric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.\\nToolformer: Language Models Can Teach Themselves to Use Tools. InAdvances\\nin Neural Information Processing Systems 36 (NeurIPS) .\\n[144] Darius Koenig Julius Lipp Sean Lee, Aamir Shakir. 2024. Open Source Strikes\\nBread - New Fluffy Embeddings Model . https://www.mixedbread.ai/blog/mxbai-\\nembed-large-v1\\n[145] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu\\nChen. 2023. Enhancing Retrieval-Augmented Large Language Models with\\nIterative Retrieval-Generation Synergy. In Findings of the Association for Com-\\nputational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023 , Houda\\nBouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational\\nLinguistics, 9248–9274.\\n[146] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike\\nLewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: Retrieval-\\nAugmented Black-Box Language Models. CoRR abs/2301.12652 (2023).\\n[147] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.\\nRetrieval Augmentation Reduces Hallucination in Conversation. In Findings of\\nthe Association for Computational Linguistics (EMNLP) . Association for Compu-\\ntational Linguistics, 3784–3803.\\n[148] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won\\nChung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen\\nPfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal\\nSchärli, Aakanksha Chowdhery, Philip Andrew Mansfield, Blaise Agüera y\\nArcas, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Katherine Chou,\\nJuraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral,\\nChristopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. 2023. Large\\nLanguage Models Encode Clinical Knowledge.Nature 620, 7972 (2023), 172–180.\\n[149] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou,\\nKevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaek-\\nermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Andrew Mans-\\nfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Agüera y\\nArcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara\\nMahdavi, Joelle K. Barral, Dale R. Webster, Gregory S. Corrado, Yossi Matias,\\nShekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023. Towards\\nExpert-Level Medical Question Answering with Large Language Models. CoRR\\nabs/2305.09617 (2023).\\n[150] Spotify. 2017. Annoy. https://github.com/spotify/annoy\\n[151] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng\\nLiu. 2024. RoFormer: Enhanced transformer with Rotary Position Embedding.\\nNeurocomputing 568 (2024), 127063.\\n[152] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun\\nGong, Heung-Yeung Shum, and Jian Guo. 2023. Think-on-Graph: Deep and\\nResponsible Reasoning of Large Language Model with Knowledge Graph.CoRR\\nabs/2307.07697 (2023).\\n[153] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume\\nLample. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR\\nabs/2302.13971 (2023).\\n[154] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,\\nYasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem\\nCucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier\\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan\\nZarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien\\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:\\nOpen Foundation and Fine-Tuned Chat Models. CoRR abs/2307.09288 (2023).\\n[155] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In Advances in Neural Information Processing Systems 30 (NeurIPS) .\\n5998–6008.\\n18'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19'}, page_content='[156] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry W. Wei, Jason Wei,\\nChris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc V. Le, and Thang Luong. 2023.\\nFreshLLMs: Refreshing Large Language Models with Search Engine Augmenta-\\ntion. CoRR abs/2310.03214 (2023).\\n[157] Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi,\\nand Bryan Catanzaro. 2023. InstructRetro: Instruction Tuning post Retrieval-\\nAugmented Pretraining. CoRR abs/2310.07713 (2023).\\n[158] Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad\\nShoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar,\\nand Bryan Catanzaro. 2023. Shall We Pretrain Autoregressive Language Models\\nwith Retrieval? A Comprehensive Study. In Proceedings of the 2023 Conference\\non Empirical Methods in Natural Language Processing (EMNLP) . 7763–7786.\\n[159] Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li,\\nXiangyu Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, Kun Yu, Yuxing\\nYuan, Yinghao Zou, Jiquan Long, Yudong Cai, Zhenxiang Li, Zhifeng Zhang,\\nYihua Mo, Jun Gu, Ruiyi Jiang, Yi Wei, and Charles Xie. 2021. Milvus: A Purpose-\\nBuilt Vector Data Management System. InSIGMOD ’21: International Conference\\non Management of Data . ACM, 2614–2627.\\n[160] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,\\nZhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei,\\nand Jirong Wen. 2024. A survey on large language model based autonomous\\nagents. Frontiers Comput. Sci. 18, 6 (2024), 186345.\\n[161] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu,\\nChenguang Zhu, and Michael Zeng. 2022. Training Data is More Valuable than\\nYou Think: A Simple and Effective Method by Retrieving from Training Data.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational\\nLinguistics (ACL). 3170–3179.\\n[162] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong\\nLi. 2023. Knowledge Editing for Large Language Models: A Survey. CoRR\\nabs/2310.16218 (2023).\\n[163] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao,\\nand Furu Wei. 2023. Augmenting Language Models with Long-Term Memory.\\nIn Advances in Neural Information Processing Systems 36 (NeurIPS) .\\n[164] Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei\\nHuang, and Kewei Tu. 2021. Improving Named Entity Recognition by Exter-\\nnal Context Retrieving and Cooperative Learning. In Proceedings of the 59th\\nAnnual Meeting of the Association for Computational Linguistics and the 11th\\nInternational Joint Conference on Natural Language Processing (ACL/IJCNLP) .\\n1800–1812.\\n[165] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge Guided\\nRetrieval Augmentation for Large Language Models. In Findings of the Associa-\\ntion for Computational Linguistics (EMNLP) . 10303–10315.\\n[166] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, and Graham\\nNeubig. 2023. Learning to Filter Context for Retrieval-Augmented Generation.\\nCoRR abs/2311.08377 (2023).\\n[167] Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard G. Baraniuk, and\\nAnima Anandkumar. 2023. Retrieval-based Controllable Molecule Generation.\\nIn The Eleventh International Conference on Learning Representations, (ICLR) .\\n[168] Mike Wu and Noah D. Goodman. 2018. Multimodal Generative Models for Scal-\\nable Weakly-Supervised Learning. In Advances in Neural Information Processing\\nSystems 31 (NeurIPS) . 5580–5590.\\n[169] Shangyu Wu, Ying Xiong, Yufei Cui, Xue Liu, Buzhou Tang, Tei-Wei Kuo,\\nand Chun Jason Xue. 2024. ReFusion: Improving Natural Language Under-\\nstanding with Computation-Efficient Retrieval Representation Fusion. CoRR\\nabs/2401.02993 (2024).\\n[170] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy.\\n2022. Memorizing Transformers. In The Tenth International Conference on\\nLearning Representations (ICLR) .\\n[171] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong,\\nMing Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,\\nXiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng\\nZou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen\\nCheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang,\\nand Tao Gui. 2023. The Rise and Potential of Large Language Model Based\\nAgents: A Survey. CoRR abs/2309.07864 (2023).\\n[172] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-\\nPack: Packaged Resources To Advance General Chinese Embedding. CoRR\\nabs/2309.07597 (2023).\\n[173] Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. 2024.\\nLarge Multimodal Agents: A Survey. CoRR abs/2402.15116 (2024).\\n[174] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking\\nRetrieval-Augmented Generation for Medicine. In Findings of the Association\\nfor Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek\\nSrikumar (Eds.).\\n[175] Ying Xiong, Xin Yang, Linjing Liu, Ka-Chun Wong, Qingcai Chen, Yang Xi-\\nang, and Buzhou Tang. 2023. EARA: Improving Biomedical Semantic Textual\\nSimilarity with Entity-Aligned Attention and Retrieval Augmentation. In Find-\\nings of the Association for Computational Linguistics (EMNLP) . Association for\\nComputational Linguistics, 8760–8771.\\n[176] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. RECOMP: Improving\\nRetrieval-Augmented LMs with Compression and Selective Augmentation.\\nCoRR abs/2310.04408 (2023).\\n[177] Frank F. Xu, Uri Alon, and Graham Neubig. 2023. Why do Nearest Neighbor\\nLanguage Models Work?. In Proceedings of the 40th International Conference on\\nMachine Learning (ICML) (Proceedings of Machine Learning Research) , Vol. 202.\\n38325–38341.\\n[178] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective\\nRetrieval Augmented Generation. CoRR abs/2401.15884 (2024).\\n[179] Guoxin Yu, Lemao Liu, Haiyun Jiang, Shuming Shi, and Xiang Ao. 2023.\\nRetrieval-Augmented Few-shot Text Classification. InFindings of the Association\\nfor Computational Linguistics (EMNLP) . 6721–6735.\\n[180] Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024.\\nEvaluation of Retrieval-Augmented Generation: A Survey.CoRR abs/2405.07437\\n(2024).\\n[181] Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. 2023. Augmentation-\\nAdapted Retriever Improves Generalization of Language Models as Generic Plug-\\nIn. In Proceedings of the 61st Annual Meeting of the Association for Computational\\nLinguistics (ACL). 2421–2436.\\n[182] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,\\nZhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\\nYufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao\\nDong, and Jie Tang. 2023. GLM-130B: An Open Bilingual Pre-trained Model.\\nIn The Eleventh International Conference on Learning Representations (ICLR) .\\nOpenReview.net.\\n[183] Biao Zhang and Rico Sennrich. 2019. Root Mean Square Layer Normalization. In\\nAdvances in Neural Information Processing Systems 32 (NeurIPS) . 12360–12371.\\n[184] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming\\nChen, Jianquan Li, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan,\\nBenyou Wang, and Haizhou Li. 2023. HuatuoGPT, Towards Taming Language\\nModel to Be a Doctor. InFindings of the Association for Computational Linguistics\\n(EMNLP). Association for Computational Linguistics, 10859–10885.\\n[185] Jianyi Zhang, Aashiq Muhamed, Aditya Anantharaman, Guoyin Wang,\\nChangyou Chen, Kai Zhong, Qingjun Cui, Yi Xu, Belinda Zeng, Trishul Chilimbi,\\nand Yiran Chen. 2023. ReAugKD: Retrieval-Augmented Knowledge Distillation\\nFor Pre-trained Language Models. In Proceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (ACL) . 1128–1136.\\n[186] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru\\nWang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng,\\nZiwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei\\nLiang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and Huajun Chen. 2024. A\\nComprehensive Study of Knowledge Editing for Large Language Models. CoRR\\nabs/2401.01286 (2024).\\n[187] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan\\nXia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. 2024. LLM as a Mas-\\ntermind: A Survey of Strategic Reasoning with Large Language Models. CoRR\\nabs/2404.01230 (2024).\\n[188] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu,\\nZhenhua Dong, and Ji-Rong Wen. 2024. A Survey on the Memory Mechanism\\nof Large Language Model based Agents. CoRR abs/2404.13501 (2024).\\n[189] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng,\\nFangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024. Retrieval-\\nAugmented Generation for AI-Generated Content: A Survey. CoRR\\nabs/2402.19473 (2024).\\n[190] Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang, Boxing Chen, Weihua\\nLuo, and Jiajun Chen. 2021. Adaptive Nearest Neighbor Machine Translation.\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Conference on Natural Language\\nProcessing (ACL/IJCNLP). 368–374.\\n[191] Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training Language Models\\nwith Memory Augmentation. In Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) . 5657–5673.\\n[192] Wenhao Zhu, Jingjing Xu, Shujian Huang, Lingpeng Kong, and Jiajun Chen.\\n2023. INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational\\nLinguistics (ACL). 15948–15959.\\n19')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "GpOQy3Jkpx7V"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "JPK4DyqMp0l-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "0dIoCH1Jp31x"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever\n",
        "keyword_retriever = BM25Retriever.from_documents(documents)\n",
        "keyword_retriever.k =  3"
      ],
      "metadata": {
        "id": "pajBavB1p6OH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_retriever.get_relevant_documents(\"what is rag?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeClBAFrp9BT",
        "outputId": "e8fd565a-1194-483a-f381-5fa0cefe9de4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-c9d6bcbb937b>:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  keyword_retriever.get_relevant_documents(\"what is rag?\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11'}, page_content='generators in RAG can learn the pattern between questions and\\nanswers and infer what answers should be. For some specific QA\\ntasks where a set of reference documents is given, retrievers in RAG\\nwould retrieve the relevant documents for concatenation, and then\\ngenerators in RAG would read the context then generate the final\\nanswers via the self-attention mechanism [7, 53, 93, 133], which is\\nsimilar to solving a reading comprehension problem. Besides, Fabbri'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='or Euclidean distance, used for ranking in the recommender sys-\\ntem can also be used in RAG [47]. Apart from these metrics, some\\nworks explored more complex similarity metrics, such as optimal\\ntransport distance [23], to obtain a task-specific similarity.\\nFinally, approximate nearest neighbor (ANN) searching is\\nalso a key step in determining what knowledge should be returned\\nas nearest neighbors. Advanced ANN searching aims to acceler-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='and manages data as a collection of key-value pairs, where keys\\nare the unique identifiers of high-dimensional embeddings and\\nvalues are the domain-specific knowledge. Since the amount of\\nthe data stored in the datastore may be quite large, the storage\\nengine, such as LMDB [108] or RocksDB [37], should be capable\\nof efficient retrieval and data persistence. The key point in the\\ndatastore for ANN search is what should be used to be stored')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[retriever, keyword_retriever], weights=[0.5, 0.5])"
      ],
      "metadata": {
        "id": "NVFcGDNQqjEH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever.get_relevant_documents(\"what is rag\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL2FH9PEqv0E",
        "outputId": "b069753f-4a12-4d91-b266-288e557a7c88"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'trapped': '/False', 'source': '/content/rag.pdf', 'producer': 'pdfTeX-1.40.25', 'keywords': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'moddate': '2025-03-04T01:53:02+00:00', 'total_pages': 19, 'page': 9, 'page_label': '10', 'subject': '', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'creationdate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX'}, page_content='7 RAG EVALUATION AND BENCHMARK\\nRetrieval-Augmented Generation (RAG) systems integrate the capa-\\nbilities of large language models (LLMs) with external knowledge\\nretrieval to advance text generation tasks in natural language pro-\\ncessing (NLP). Evaluating RAG systems requires a dual focus on\\nretrieval quality and generation performance to ensure the system\\neffectively leverages external knowledge and produces accurate,\\ncontextually appropriate outputs. Recent research has introduced'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11'}, page_content='generators in RAG can learn the pattern between questions and\\nanswers and infer what answers should be. For some specific QA\\ntasks where a set of reference documents is given, retrievers in RAG\\nwould retrieve the relevant documents for concatenation, and then\\ngenerators in RAG would read the context then generate the final\\nanswers via the self-attention mechanism [7, 53, 93, 133], which is\\nsimilar to solving a reading comprehension problem. Besides, Fabbri'),\n",
              " Document(metadata={'moddate': '2025-03-04T01:53:02+00:00', 'producer': 'pdfTeX-1.40.25', 'subject': '', 'page_label': '14', 'keywords': '', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'total_pages': 19, 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/rag.pdf', 'creationdate': '2025-03-04T01:53:02+00:00', 'page': 13, 'trapped': '/False'}, page_content='RAG training, including RAG with/without datastore update. Then,\\nthis paper presents RAG evaluation and benchmarking, and ex-\\nplores how RAG can be adapted for various NLP tasks and provides\\npractical applications of RAG in real-world scenarios. Conclusively,\\n14'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='or Euclidean distance, used for ranking in the recommender sys-\\ntem can also be used in RAG [47]. Apart from these metrics, some\\nworks explored more complex similarity metrics, such as optimal\\ntransport distance [23], to obtain a task-specific similarity.\\nFinally, approximate nearest neighbor (ANN) searching is\\nalso a key step in determining what knowledge should be returned\\nas nearest neighbors. Advanced ANN searching aims to acceler-'),\n",
              " Document(metadata={'creationdate': '2025-03-04T01:53:02+00:00', 'moddate': '2025-03-04T01:53:02+00:00', 'producer': 'pdfTeX-1.40.25', 'subject': '', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'trapped': '/False', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page': 0, 'keywords': '', 'page_label': '1', 'source': '/content/rag.pdf', 'total_pages': 19, 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey'}, page_content='edge. RAG can also convert a general LLM into a domain-specific\\nLLM by constructing and utilizing a domain-specific knowledge\\ndatabase. Therefore, RAG plays an important role in augmenting the\\nfunctionality of LLMs, making them more accurate, knowledgeable,\\nand reliable in a wide range of applications.\\nContributions: This paper reviews all techniques involved in\\nRAG for natural language processing. Although there are several\\nsurvey papers for RAG [44, 63, 96, 180, 189], our survey still has'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False', 'source': '/content/rag.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='and manages data as a collection of key-value pairs, where keys\\nare the unique identifiers of high-dimensional embeddings and\\nvalues are the domain-specific knowledge. Since the amount of\\nthe data stored in the datastore may be quite large, the storage\\nengine, such as LMDB [108] or RocksDB [37], should be capable\\nof efficient retrieval and data persistence. The key point in the\\ndatastore for ANN search is what should be used to be stored'),\n",
              " Document(metadata={'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'creationdate': '2025-03-04T01:53:02+00:00', 'keywords': '', 'producer': 'pdfTeX-1.40.25', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'moddate': '2025-03-04T01:53:02+00:00', 'total_pages': 19, 'page': 11, 'source': '/content/rag.pdf', 'subject': '', 'page_label': '12', 'trapped': '/False'}, page_content='reflect the latest developments. This approach enhances the agent’s\\nability to provide accurate and timely information, improving its\\neffectiveness in dynamic environments.\\nIn both cases, RAG plays a crucial role in augmenting the capabil-\\nities of LLM-based agents by enabling them to access and leverage\\na wider range of information, whether it’s from their own external\\nmemory or from real-time sources on the web. This leads to more\\ninformed decision-making and enhances the overall performance')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI()"
      ],
      "metadata": {
        "id": "TDe5qxriq1Xm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "template = \"\"\"\"\n",
        "You are a helpful assistant that answers questions based on the following context.\n",
        "If you don't find the answer in the context, just say that you don't know.\n",
        "Context: {context}\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "Answer:\n",
        "\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Setup RAG pipeline\n",
        "rag_chain = (\n",
        "    {\"context\": ensemble_retriever,  \"input\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "ayRt3mV_q4YR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke('what is rag?')\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CiY9bw9Wq8yW",
        "outputId": "98378534-91c4-4fa5-e295-6b2ca1366fad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval-Augmented Generation. It refers to systems that integrate large language models with external knowledge retrieval to enhance text generation tasks in natural language processing (NLP).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI()"
      ],
      "metadata": {
        "id": "nNykkPbprGJ4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langsmith import Client\n",
        "client = Client()\n",
        "prompt = client.pull_prompt(\"langchain-ai/rag-fusion-query-generation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpb9_WxSzQgG",
        "outputId": "e6930730-6903-4ce7-d862-cfd6caa76afe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_queries = (\n",
        "    rag_chain|prompt | ChatOpenAI(temperature=0) | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "id": "7bNt4d9PzTuG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    fused_scores = {}\n",
        "    for docs in results:\n",
        "        # Assumes the docs are returned in sorted order of relevance\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "    return reranked_results"
      ],
      "metadata": {
        "id": "JLvsTaJFzlSK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = generate_queries | retriever.map() | reciprocal_rank_fusion"
      ],
      "metadata": {
        "id": "nVg8uSUEzo7d"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.input_schema.schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbIa8fXmzsI-",
        "outputId": "42155258-d64c-480f-a90d-8d191b528474"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'RunnableParallel<context,input>Input', 'type': 'string'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"what is rag?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N5LB3MxCzvEf",
        "outputId": "14879e98-505b-4bad-d2cc-95c2af572980"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(metadata={'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'source': '/content/rag.pdf', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'subject': '', 'trapped': '/False', 'creationdate': '2025-03-04T01:53:02+00:00', 'total_pages': 19, 'page_label': '10', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'page': 9, 'producer': 'pdfTeX-1.40.25', 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00'}, page_content='7 RAG EVALUATION AND BENCHMARK\\nRetrieval-Augmented Generation (RAG) systems integrate the capa-\\nbilities of large language models (LLMs) with external knowledge\\nretrieval to advance text generation tasks in natural language pro-\\ncessing (NLP). Evaluating RAG systems requires a dual focus on\\nretrieval quality and generation performance to ensure the system\\neffectively leverages external knowledge and produces accurate,\\ncontextually appropriate outputs. Recent research has introduced'),\n",
              "  0.016666666666666666),\n",
              " (Document(metadata={'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'total_pages': 19, 'creationdate': '2025-03-04T01:53:02+00:00', 'page': 9, 'keywords': '', 'subject': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/rag.pdf', 'page_label': '10', 'trapped': '/False', 'moddate': '2025-03-04T01:53:02+00:00', 'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX'}, page_content='7 RAG EVALUATION AND BENCHMARK\\nRetrieval-Augmented Generation (RAG) systems integrate the capa-\\nbilities of large language models (LLMs) with external knowledge\\nretrieval to advance text generation tasks in natural language pro-\\ncessing (NLP). Evaluating RAG systems requires a dual focus on\\nretrieval quality and generation performance to ensure the system\\neffectively leverages external knowledge and produces accurate,\\ncontextually appropriate outputs. Recent research has introduced'),\n",
              "  0.016666666666666666),\n",
              " (Document(metadata={'moddate': '2025-03-04T01:53:02+00:00', 'subject': '', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'page_label': '10', 'trapped': '/False', 'total_pages': 19, 'page': 9, 'creationdate': '2025-03-04T01:53:02+00:00', 'keywords': '', 'source': '/content/rag.pdf', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.25'}, page_content='7 RAG EVALUATION AND BENCHMARK\\nRetrieval-Augmented Generation (RAG) systems integrate the capa-\\nbilities of large language models (LLMs) with external knowledge\\nretrieval to advance text generation tasks in natural language pro-\\ncessing (NLP). Evaluating RAG systems requires a dual focus on\\nretrieval quality and generation performance to ensure the system\\neffectively leverages external knowledge and produces accurate,\\ncontextually appropriate outputs. Recent research has introduced'),\n",
              "  0.016666666666666666),\n",
              " (Document(metadata={'keywords': '', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'trapped': '/False', 'source': '/content/rag.pdf', 'page': 9, 'total_pages': 19, 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'moddate': '2025-03-04T01:53:02+00:00', 'page_label': '10', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'producer': 'pdfTeX-1.40.25', 'creationdate': '2025-03-04T01:53:02+00:00', 'subject': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey'}, page_content='7 RAG EVALUATION AND BENCHMARK\\nRetrieval-Augmented Generation (RAG) systems integrate the capa-\\nbilities of large language models (LLMs) with external knowledge\\nretrieval to advance text generation tasks in natural language pro-\\ncessing (NLP). Evaluating RAG systems requires a dual focus on\\nretrieval quality and generation performance to ensure the system\\neffectively leverages external knowledge and produces accurate,\\ncontextually appropriate outputs. Recent research has introduced'),\n",
              "  0.016666666666666666),\n",
              " (Document(metadata={'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'creationdate': '2025-03-04T01:53:02+00:00', 'total_pages': 19, 'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'page': 12, 'keywords': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'moddate': '2025-03-04T01:53:02+00:00', 'subject': '', 'page_label': '13', 'trapped': '/False', 'source': '/content/rag.pdf'}, page_content='RAG by ensuring that the language models are not only generating\\ntext based on their internal knowledge but are also capable of\\npulling in external data to provide responses that are contextually\\nenriched and informationally robust.\\n10 DISCUSSION AND FUTURE DIRECTION\\nDespite the success of the RAG for natural language processing,\\nsome challenges should be considered. This paper highlights these\\nchallenges to inspire future research and provides possible future'),\n",
              "  0.01639344262295082),\n",
              " (Document(metadata={'creationdate': '2025-03-04T01:53:02+00:00', 'source': '/content/rag.pdf', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'moddate': '2025-03-04T01:53:02+00:00', 'trapped': '/False', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'producer': 'pdfTeX-1.40.25', 'page_label': '13', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'total_pages': 19, 'page': 12, 'subject': '', 'keywords': ''}, page_content='RAG by ensuring that the language models are not only generating\\ntext based on their internal knowledge but are also capable of\\npulling in external data to provide responses that are contextually\\nenriched and informationally robust.\\n10 DISCUSSION AND FUTURE DIRECTION\\nDespite the success of the RAG for natural language processing,\\nsome challenges should be considered. This paper highlights these\\nchallenges to inspire future research and provides possible future'),\n",
              "  0.01639344262295082),\n",
              " (Document(metadata={'source': '/content/rag.pdf', 'trapped': '/False', 'moddate': '2025-03-04T01:53:02+00:00', 'page_label': '11', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page': 10, 'total_pages': 19, 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'creationdate': '2025-03-04T01:53:02+00:00', 'keywords': '', 'subject': '', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX'}, page_content='RAG techniques combine information retrieval with model-based\\ngeneration, which is highly suitable for QA systems. In particu-\\nlar, open-domain QA systems usually first require searching for\\nknowledge from the Internet or large-scale databases, then generate\\nthe corresponding answers according to the retrieved knowledge.\\nNaturally, given similar questions and corresponding answers as\\ndemonstrations which are concatenated into inputs [64, 98, 161],'),\n",
              "  0.01639344262295082),\n",
              " (Document(metadata={'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page': 13, 'creationdate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'total_pages': 19, 'moddate': '2025-03-04T01:53:02+00:00', 'subject': '', 'page_label': '14', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'source': '/content/rag.pdf', 'producer': 'pdfTeX-1.40.25', 'keywords': '', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'trapped': '/False'}, page_content='In this survey, we delve into the development of RAG within the\\nfield of natural language processing. First, this paper introduces\\nthe components of RAG and their functionalities. Subsequently,\\nthis paper elaborates on each step involved in retriever, discussing\\nthe diverse techniques. Furthermore, this paper categorizes the\\nretrieval fusions, evaluating the strengths and weaknesses inherent\\nin each retrieval fusion technique. Besides, this paper discusses the'),\n",
              "  0.01639344262295082),\n",
              " (Document(metadata={'keywords': '', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/rag.pdf', 'page': 13, 'creationdate': '2025-03-04T01:53:02+00:00', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'page_label': '14', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'producer': 'pdfTeX-1.40.25', 'subject': '', 'total_pages': 19, 'moddate': '2025-03-04T01:53:02+00:00'}, page_content='In this survey, we delve into the development of RAG within the\\nfield of natural language processing. First, this paper introduces\\nthe components of RAG and their functionalities. Subsequently,\\nthis paper elaborates on each step involved in retriever, discussing\\nthe diverse techniques. Furthermore, this paper categorizes the\\nretrieval fusions, evaluating the strengths and weaknesses inherent\\nin each retrieval fusion technique. Besides, this paper discusses the'),\n",
              "  0.016129032258064516),\n",
              " (Document(metadata={'total_pages': 19, 'page_label': '12', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'moddate': '2025-03-04T01:53:02+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'source': '/content/rag.pdf', 'page': 11, 'producer': 'pdfTeX-1.40.25', 'subject': '', 'keywords': '', 'trapped': '/False'}, page_content='own external memory [56, 114, 188]. This external memory serves\\nas a knowledge base that the agent can draw upon to enhance its\\nunderstanding and decision-making. When faced with a query or a\\ntask, the agent can use RAG to retrieve relevant information from\\nthis memory, which is then integrated into the generation process\\nof the LLM. This allows the agent to produce responses or solutions\\nthat are informed by a wider range of knowledge, leading to more'),\n",
              "  0.016129032258064516),\n",
              " (Document(metadata={'source': '/content/rag.pdf', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'total_pages': 19, 'page_label': '2', 'page': 1, 'producer': 'pdfTeX-1.40.25', 'keywords': '', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-03-04T01:53:02+00:00', 'trapped': '/False', 'moddate': '2025-03-04T01:53:02+00:00', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'subject': ''}, page_content='introduce all technical details used in retrievers and retrieval fu-\\nsions. Section 6 presents how to train the RAG with/without new\\nknowledge. Section 8 presents the techniques used in representative\\nNLP tasks. Section 9 shows the applications of RAG in practical\\nNLP scenarios. Section 10 discusses the future directions of RAG.\\nSection 11 makes a final conclusion of this paper.\\n2 OVERVIEW OF RETRIEVAL-AUGMENTED\\nGENERATION\\nThis section gives an overview of Retrieval-Augmented Generation'),\n",
              "  0.016129032258064516),\n",
              " (Document(metadata={'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'page_label': '13', 'moddate': '2025-03-04T01:53:02+00:00', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'keywords': '', 'creationdate': '2025-03-04T01:53:02+00:00', 'total_pages': 19, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page': 12, 'producer': 'pdfTeX-1.40.25', 'trapped': '/False', 'source': '/content/rag.pdf', 'subject': '', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX'}, page_content='RAG by ensuring that the language models are not only generating\\ntext based on their internal knowledge but are also capable of\\npulling in external data to provide responses that are contextually\\nenriched and informationally robust.\\n10 DISCUSSION AND FUTURE DIRECTION\\nDespite the success of the RAG for natural language processing,\\nsome challenges should be considered. This paper highlights these\\nchallenges to inspire future research and provides possible future'),\n",
              "  0.016129032258064516),\n",
              " (Document(metadata={'total_pages': 19, 'trapped': '/False', 'page': 1, 'subject': '', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.25', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'page_label': '2', 'creationdate': '2025-03-04T01:53:02+00:00', 'source': '/content/rag.pdf', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': '2025-03-04T01:53:02+00:00', 'keywords': ''}, page_content='introduce all technical details used in retrievers and retrieval fu-\\nsions. Section 6 presents how to train the RAG with/without new\\nknowledge. Section 8 presents the techniques used in representative\\nNLP tasks. Section 9 shows the applications of RAG in practical\\nNLP scenarios. Section 10 discusses the future directions of RAG.\\nSection 11 makes a final conclusion of this paper.\\n2 OVERVIEW OF RETRIEVAL-AUGMENTED\\nGENERATION\\nThis section gives an overview of Retrieval-Augmented Generation'),\n",
              "  0.015873015873015872),\n",
              " (Document(metadata={'keywords': '', 'source': '/content/rag.pdf', 'trapped': '/False', 'creationdate': '2025-03-04T01:53:02+00:00', 'subject': '', 'page': 11, 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'total_pages': 19, 'producer': 'pdfTeX-1.40.25', 'moddate': '2025-03-04T01:53:02+00:00', 'page_label': '12'}, page_content='vides agents with the capability to incorporate external knowledge\\nover extended periods. Therefore, applying RAG would be bene-\\nficial to access a broader range of information, improving agents’\\ndecision-making and problem-solving abilities [187]. This section\\nexplores how LLM-based agents can leverage RAG from two per-\\nspectives.\\nUsing RAG to Retrieve from External Memory. LLM-based\\nagents can utilize RAG to access and retrieve information from their'),\n",
              "  0.015873015873015872),\n",
              " (Document(metadata={'source': '/content/rag.pdf', 'keywords': '', 'trapped': '/False', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'subject': '', 'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'producer': 'pdfTeX-1.40.25', 'creationdate': '2025-03-04T01:53:02+00:00', 'moddate': '2025-03-04T01:53:02+00:00', 'total_pages': 19, 'page': 0, 'page_label': '1', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'}, page_content='cially in the retriever and the retrieval fusions. Besides, tutorial\\ncodes are provided for implementing the representative techniques\\nin RAG. This paper further discusses the RAG update, including\\nRAG with/without knowledge update. Then, we introduce RAG\\nevaluation and benchmarking, as well as the application of RAG in\\nrepresentative NLP tasks and industrial scenarios. Finally, this pa-\\nper discusses RAG’s future directions and challenges for promoting\\nthis field’s development.'),\n",
              "  0.015873015873015872),\n",
              " (Document(metadata={'author': 'Shangyu Wu, Ying Xiong*, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue', 'title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'total_pages': 19, 'keywords': '', 'moddate': '2025-03-04T01:53:02+00:00', 'source': '/content/rag.pdf', 'trapped': '/False', 'creationdate': '2025-03-04T01:53:02+00:00', 'page': 9, 'subject': '', 'page_label': '10', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.25'}, page_content='while maintaining high retrieval and generation quality standards.\\n8 TASKS\\nThis section lists several classical tasks in the NLP domain and\\nintroduces advanced RAG techniques used to solve these tasks.\\n8.1 Language Modeling\\nLanguage modeling is the task that requires the prediction of the\\nprobability distribution of the next word or character given a se-\\nquence of words or characters, which is also named the next-token\\nprediction task. Language modeling has become the fundamental'),\n",
              "  0.015873015873015872)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context.\n",
        "If you don't find the answer in the context, just say that you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "rag_fusion_chain = (\n",
        "    {\n",
        "        \"context\": chain,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "duAzjnkm0ZXr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_fusion_chain.invoke(\"what is rag and explain it's importance?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "sv-GDAqr0asq",
        "outputId": "a300be86-0fef-41dd-c05b-3a4a0640c03a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval-Augmented Generation. It is a system that combines large language models with external knowledge retrieval to enhance text generation tasks in natural language processing. RAG is important because it allows the system to effectively leverage external knowledge, produce accurate outputs, and ensure contextually appropriate responses by integrating external data into the generation process. It helps in making language models more knowledgeable, accurate, and reliable in various applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_x14_EQd1Ayh",
        "outputId": "f978f3e2-83f3-4e48-e2b7-60e994743347"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "data = {\n",
        "    \"question\": [\"What is RAG?\"],\n",
        "    \"contexts\": [[\n",
        "        \"RAG (Retrieval-Augmented Generation) is a method that combines retrieval-based and generation-based models. It retrieves relevant documents and then uses them to generate more accurate and grounded responses.\"\n",
        "    ]],\n",
        "    \"answer\": [\"RAG is a method that retrieves documents and then generates responses based on them.\"],\n",
        "    \"ground_truth\": [\"RAG is a method that combines document retrieval with text generation to improve accuracy and grounding in responses.\"]\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data)"
      ],
      "metadata": {
        "id": "0Oggfvhs0-UN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ragas"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5N3GRsJ1MzG",
        "outputId": "0c9dd569-d052-4142-e563-765a9881dde2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ragas\n",
            "  Downloading ragas-0.2.15-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ragas) (2.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from ragas) (3.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from ragas) (0.9.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragas) (0.3.24)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (from ragas) (0.3.55)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (from ragas) (0.3.22)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (from ragas) (0.3.14)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ragas) (1.6.0)\n",
            "Collecting appdirs (from ragas)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from ragas) (2.11.3)\n",
            "Requirement already satisfied: openai>1 in /usr/local/lib/python3.11/dist-packages (from ragas) (1.75.0)\n",
            "Collecting diskcache>=5.6.3 (from ragas)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ragas) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ragas) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ragas) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->ragas) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (6.0.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas) (0.3.33)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas) (2.0.40)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core->ragas) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core->ragas) (1.33)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community->ragas) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->ragas) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->ragas) (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->ragas) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->ragas) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->ragas) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.1.0)\n",
            "Downloading ragas-0.2.15-py3-none-any.whl (190 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: appdirs, diskcache, ragas\n",
            "Successfully installed appdirs-1.4.4 diskcache-5.6.3 ragas-0.2.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.evaluation import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision\n",
        ")\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "91f5cf482d784e75b68b0af8af45ed86",
            "8d269b1eae6342f793e893bf58fa5664",
            "cdf03142bf7f4ddb933bd3ca8e6a6519",
            "d6f9e1c9e816431dbfd5abec80c26cd4",
            "8324e1fcdded407abcc03718d3703b36",
            "e750e09fd8ea419bad022c56a0b82863",
            "20e1baa8f3aa46d8812e60ec36d2221e",
            "c2c6dea68e2946a8b521538188d6be9f",
            "4f0b73e9f40d4e8e87dc0dedc9e9ef7b",
            "d8cacfaa7c6e4d02b9c973b2503ffd4d",
            "421a88555243446f8e395c9dd83e1ab2"
          ]
        },
        "id": "RSOykacS1IAE",
        "outputId": "ea3214be-6604-4ee9-a7b2-2b02e41e0864"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91f5cf482d784e75b68b0af8af45ed86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'context_precision': 1.0000, 'context_recall': 1.0000, 'faithfulness': 1.0000, 'answer_relevancy': 1.0000}\n"
          ]
        }
      ]
    }
  ]
}